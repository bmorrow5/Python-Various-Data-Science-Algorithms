{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brandon L Morrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. RBF Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first develop pseudocode for our rbf nn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Part 1 Response ##\n",
    "Import libraries\n",
    "\n",
    "def rbf_nn(spread=0.5):\n",
    "    \n",
    "    Import the iris data\n",
    "    setosa = iris.data.toNumpy(replace setosa = 1, replace versicolor = -1, replace virginica = -1)\n",
    "    versicolor = iris.data.toNumpy(replace setosa = -1, replace versicolor = 1, replace virginica = -1)\n",
    "    virginica = iris.data.toNumpy(replace setosa = -1, replace versicolor = -1, replace virginica = 1)\n",
    "\n",
    "    # Train the models\n",
    "    model1 = rbf_train_no_bias(dataX, classY, spread)\n",
    "    model2 = rbf_train_no_bias(dataX, classY, spread)\n",
    "    model3 = rbf_train_no_bias(dataX, classY, spread)\n",
    "    models = model1 + model2 + model3\n",
    "\n",
    "    # Now classify using the trained models\n",
    "    ypred1 = rbf_classify(x0, model1)\n",
    "    ypred2 = rbf_classify(x0, model1)\n",
    "    ypred3 = rbf_classify(x0, model1)\n",
    "    predictions = ypred1 + ypred2 + ypred3\n",
    "\n",
    "    true_classes = iris.data.toNumpy(replace setosa = 0, replace versicolor = 1, replace virginica = 2)\n",
    "    predicted_classes = argmax(predictions) # will give the locataion index of where the 1 is and will ignore -1\n",
    "\n",
    "    # now lets compare the true class with the predicted class\n",
    "    accuracy = np.sum(true_classes == pred_classes) / len(true_classes)\n",
    "\n",
    "    # Now lets get the indices of the misclassified sections\n",
    "    misclassified = np.where(true_classes != pred_classes) # get indexes of misclassifications\n",
    "\n",
    "    display misclassified indices and accuracy score"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now we will build our RBF train algorithm\n",
    "\n",
    "def rbf_train_no_bias(x, y, spread=0.5):\n",
    "    n, d = x.shape\n",
    "    h = np.zeros(n,n) # create our matrix\n",
    "    \n",
    "    for j in range n\n",
    "        w = [j,:]\n",
    "        for i in range n \n",
    "            h[i,j] = gaussian radial basis function\n",
    "            \n",
    "    # Now calculate the weight vector\n",
    "    w_hat = (h.inverse @ y) # calculate weights from hidden layer values (will be more complex but for pseudocode\n",
    "    yt = (h @ w_hat)  # Equation 8 sum the weights no bias\n",
    "    ypred = np.ones_like(y) \n",
    "    ypred[np.where(yt < 0)] = -1 \n",
    "    pred_error = 1 - np.sum(y == ypred) / y.size\n",
    "            \n",
    "    model = {'w_hat': w_hat,'w': x,'spread': spread,'error': pred_error}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Classifies using a trained model from our rbf train function\n",
    "\n",
    "def rbf_classify_no_bias(x, model):\n",
    "    \n",
    "    spread = model['spread']\n",
    "    n1, d1 = x.shape\n",
    "    n2, d2 = model['w'].shape\n",
    "    h = np.zeros(n1, n2)\n",
    "    \n",
    "    for j in range(n2):\n",
    "        w = model['w'][j, :]\n",
    "        for i in range(n1):\n",
    "            h[i, j] = gaussian radial basis function\n",
    "    y = (h @ model['w_hat']).T\n",
    "    ypred = np.ones_like(y)\n",
    "    ypred[np.where(y < 0)] = -1\n",
    "\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analyze your design<br><br>\n",
    "    - Calculate the running time of the system above in O-notation.\n",
    "    - Calculate the total running time of the above system as T(n) with each line of pseu-\n",
    "    docode or code accounted for.\n",
    "    - How does the total running time T(n) compare to the running time in O-notation?<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Due to the nested for loops to calculate the gaussian radial basis function the time complexity will be O(n^3) due to the inverse that occurs on pinv to calculate the w_hat vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By line T(n):  ##\n",
    "Import libraries\n",
    "\n",
    "def rbf_nn(spread=0.5):\n",
    "    \n",
    "    Import the iris data\n",
    "    setosa = iris.data.toNumpy(replace setosa = 1, replace versicolor = -1, replace virginica = -1) \n",
    "    versicolor = iris.data.toNumpy(replace setosa = -1, replace versicolor = 1, replace virginica = -1)\n",
    "    virginica = iris.data.toNumpy(replace setosa = -1, replace versicolor = -1, replace virginica = 1)\n",
    "    T(n) = +4\n",
    "\n",
    "    # Train the models\n",
    "    model1 = rbf_train_no_bias(dataX, classY, spread) T(n) = n^3 + n^2 + 2n + 8\n",
    "    model2 = rbf_train_no_bias(dataX, classY, spread) T(n) = n^3 + n^2 + 2n + 8\n",
    "    model3 = rbf_train_no_bias(dataX, classY, spread) T(n) = n^3 + n^2 + 2n + 8 \n",
    "    models = model1 + model2 + model3\n",
    "    T(n) = 3n^3 + 3n^2 + 6n + 28\n",
    "\n",
    "    # Now classify using the trained models\n",
    "    ypred1 = rbf_classify(x0, model1) T(n) = n^3 + n^2 + 2n + 7 \n",
    "    ypred2 = rbf_classify(x0, model1) T(n) = n^3 + n^2 + 2n + 7 \n",
    "    ypred3 = rbf_classify(x0, model1) T(n) = n^3 + n^2 + 2n + 7 \n",
    "    predictions = ypred1 + ypred2 + ypred3\n",
    "    T(n) = 3n^3 + 3n^2 + 6n + 25\n",
    "\n",
    "    true_classes = iris.data.toNumpy(replace setosa = 0, replace versicolor = 1, replace virginica = 2)\n",
    "    predicted_classes = argmax(predictions) # will give the locataion index of where the 1 is and will ignore -1\n",
    "    T(n) = 2\n",
    "\n",
    "    # now lets compare the true class with the predicted class\n",
    "    accuracy = np.sum(true_classes == pred_classes) / len(true_classes)\n",
    "    T(n) = 1\n",
    "\n",
    "    # Now lets get the indices of the misclassified sections\n",
    "    misclassified = np.where(true_classes != pred_classes) # get indexes of misclassifications\n",
    "    T(n) = 1\n",
    "\n",
    "    display misclassified indices and accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## T(n) = 6n^3 + 6n^2 + 12n + 61\n",
    "## O(n) = n^3\n",
    "This is about what we would expect with the neural network creation and manipulation of the matrices. I will be able to confirm the O(n) number of 12 once coding is complete from pseudocode, but this is an estimation of the complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will Implement our RBF NN using Python<br>\n",
    "- Train three two class models using the Iris data set as input training data, the Iris\n",
    "    data will need to be reconfigured as a one-vs-all or one-vs-one data set.\n",
    "- Process the test data set to determine which class each test observation belongs to,\n",
    "    in this problem you will simply use all 150 observations as your test data.\n",
    "- What is the classification accuracy of your design?\n",
    "- If you had any misclassifications what was the cause of this, e.g, did the spread have an impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recieves an x of [150,4] and a y of [150,] of iris data and trains a weighted vector\n",
    "# Using linear least squares\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def rbf_train_no_bias(x, y, spread=0.5):\n",
    "    \n",
    "    n, d = x.shape\n",
    "    h = np.zeros((n, n))\n",
    "    for j in range(n):\n",
    "        w = x[j, :]\n",
    "        for i in range(n):\n",
    "            h[i, j] = np.exp(-np.sum(np.square(x[i, :] - w)) / (2 * spread ** 2)) # H is [150x150]\n",
    "    \n",
    "    # Now lets calculate the weight vector\n",
    "    w_hat = np.linalg.pinv(h.T @ h) @ h.T @ y # w_hat is [150,]\n",
    "    yt = (h @ w_hat).T # yt has values of -.999 and .999 but will correct with ypred\n",
    "    ypred = np.ones_like(y) \n",
    "    ypred[np.where(yt < 0)] = -1\n",
    "    pred_error = 1 - np.sum(y == ypred) / y.size\n",
    "    model = {'w_hat': w_hat,'w': x,'spread': spread,'error': pred_error}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classifies using a trained model from our rbf train function\n",
    "def rbf_classify_no_bias(x, model):\n",
    "    \n",
    "    spread = model['spread']\n",
    "    n1, d1 = x.shape\n",
    "    n2, d2 = model['w'].shape\n",
    "    h = np.zeros((n1, n2))\n",
    "    \n",
    "    for j in range(n2):\n",
    "        w = model['w'][j, :]\n",
    "        for i in range(n1):\n",
    "            h[i, j] = np.exp(-np.dot(x[i, :]-w, x[i, :]-w) / (2*spread**2))\n",
    "    y = (h @ model['w_hat']).T\n",
    "    ypred = np.ones_like(y)\n",
    "    ypred[np.where(y < 0)] = -1\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spread: 0.5\n",
      "Classification accuracy: 100.0%\n",
      "Misclassified: 0\n",
      "Misclassified Indices: []\n",
      "\n",
      "Predicted valuess: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Actual Values: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Spread: 55\n",
      "Classification accuracy: 97.33333333333334%\n",
      "Misclassified: 4\n",
      "Misclassified Indices: [ 70  83 119 133]\n",
      "\n",
      "Predicted valuess: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Actual Values: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Spread: 100\n",
      "Classification accuracy: 94.66666666666667%\n",
      "Misclassified: 8\n",
      "Misclassified Indices: [ 70 106 119 123 126 133 134 146]\n",
      "\n",
      "Predicted valuess: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2\n",
      " 2 2]\n",
      "\n",
      "Actual Values: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n",
      "Spread: 500\n",
      "Classification accuracy: 74.0%\n",
      "Misclassified: 39\n",
      "Misclassified Indices: [ 50  51  52  54  56  59  61  64  65  66  70  71  74  75  77  78  84  85\n",
      "  86  88  91  95  96  97  98  99 103 105 106 107 108 118 119 122 129 130\n",
      " 133 134 146]\n",
      "\n",
      "Predicted valuess: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 1 2 1 1 0 1 0 1 1 0 0 2 1 1 1 2 0 1 1\n",
      " 0 0 1 2 0 1 1 1 1 1 2 2 0 1 0 1 1 0 1 1 1 0 0 0 0 0 2 2 2 1 2 1 1 1 1 2 2\n",
      " 2 2 2 2 2 2 2 1 1 2 2 1 2 2 2 2 2 2 1 1 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2\n",
      " 2 2]\n",
      "\n",
      "Actual Values: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Type the response for part 2 here ##\n",
    "\n",
    "def rbf_nn(spread=0.5):\n",
    "\n",
    "    iris_df = pd.read_csv(\"iris.csv\")\n",
    "    iris_x = iris_df.iloc[:, :-1].values # Will be [150 x 4]\n",
    "\n",
    "    #Now create one vs all arrays that are [150,]\n",
    "    iris_setosa = iris_df.iloc[:, -1].replace({'setosa': 1, 'versicolor': -1, 'virginica': -1}).values\n",
    "    iris_versicolor = iris_df.iloc[:, -1].replace({'setosa': -1, 'versicolor': 1, 'virginica': -1}).values\n",
    "    iris_virginica = iris_df.iloc[:, -1].replace({'setosa': -1, 'versicolor': -1, 'virginica': 1}).values\n",
    "    iris_y = [iris_setosa, iris_versicolor, iris_virginica]\n",
    "\n",
    "    # Train the models\n",
    "    model1 = rbf_train_no_bias(iris_x, iris_setosa, spread)\n",
    "    model2 = rbf_train_no_bias(iris_x, iris_versicolor, spread)\n",
    "    model3 = rbf_train_no_bias(iris_x, iris_virginica, spread)\n",
    "    models = [model1, model2, model3]\n",
    "\n",
    "    pred1 = rbf_classify_no_bias(iris_x, model1)\n",
    "    pred2 = rbf_classify_no_bias(iris_x, model2)\n",
    "    pred3 = rbf_classify_no_bias(iris_x, model3)\n",
    "    predictions = np.vstack((pred1, pred2, pred3))\n",
    "\n",
    "    true_classes = iris_df.iloc[:, -1].replace({'setosa': 0, 'versicolor': 1, 'virginica': 2}).values\n",
    "    pred_classes = np.argmax(predictions, axis=0)\n",
    "\n",
    "\n",
    "    accuracy = np.sum(true_classes == pred_classes) / len(true_classes)\n",
    "    misclassified = np.where(true_classes != pred_classes)[0] # get indexes of misclassifications\n",
    "\n",
    "    print(f'Spread: {spread}')\n",
    "    print(f'Classification accuracy: {accuracy * 100}%')\n",
    "    print(f'Misclassified: {len(misclassified)}')\n",
    "    print(f'Misclassified Indices: {misclassified}\\n')\n",
    "    print(f'Predicted valuess: {pred_classes}\\n')\n",
    "    print(f'Actual Values: {true_classes}\\n')\n",
    "\n",
    "rbf_nn(spread=0.5)\n",
    "rbf_nn(spread=55)\n",
    "rbf_nn(spread=100)\n",
    "rbf_nn(spread=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The spread absolutely has an impact on the accuracy of the classification and with a larger spread there leads to less accuracy. As we saw on HW3 when using the gaussian kernel function to graph the classification accuracy decreases as spread increases. This is due to underfitting as the gaussian classification zone becommes more and more broad as the spread/sigma is increased which leads to a misclassification of values. To test and demonstrate this I turned the rbf_nn into a function to run multiple iterations at different spreads ##"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
