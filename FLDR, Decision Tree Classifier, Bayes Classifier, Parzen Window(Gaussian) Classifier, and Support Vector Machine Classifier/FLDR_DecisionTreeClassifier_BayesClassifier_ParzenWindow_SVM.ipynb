{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brandon L Morrow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Feature Ranking Methods (FLDR, Decision Tree Classification)<br>\n",
    "1. Using the MNIST dataset, rank the features using Fisher’s Linear Discriminant Ratio as the criteria (you may utilize your implementation from the previous assignment).\n",
    "2. For part 2, you will create and fit a decision tree classifier (using the scikit learn built in function) offering a new avenue for feature ranking.<br>\n",
    "    (a) Create an algorithm that will accept your dataset and fit a decision tree classifier\n",
    "to it. <br>\n",
    "(b) Using your algorithm in part 2.1, run a 5 fold cross validation and score the\n",
    "model based on its ability to classify accurately.<br>\n",
    "(c) From the model you fit, extract the feature importances.<br>\n",
    "(d) Add unit testing for the feature ranking in both methods.<br>\n",
    "(e) Plot your decision tree graph and examine the key parameters displayed. Offer\n",
    "an analysis for how the decision tree runs its predict method along with an analysis of the\n",
    "runtime complexity.<br>\n",
    "(f) Compare the features importances from the decision tree classifier with the feature\n",
    "ranking using FLDR. Consider what sets the two methods apart and offer a discussion on\n",
    "the benefits of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Dataframe: (42000, 785)\n",
      "Head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tail:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "41995      0       0       0       0       0       0       0       0       0   \n",
       "41996      1       0       0       0       0       0       0       0       0   \n",
       "41997      7       0       0       0       0       0       0       0       0   \n",
       "41998      6       0       0       0       0       0       0       0       0   \n",
       "41999      9       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "41995       0  ...         0         0         0         0         0   \n",
       "41996       0  ...         0         0         0         0         0   \n",
       "41997       0  ...         0         0         0         0         0   \n",
       "41998       0  ...         0         0         0         0         0   \n",
       "41999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "41995         0         0         0         0         0  \n",
       "41996         0         0         0         0         0  \n",
       "41997         0         0         0         0         0  \n",
       "41998         0         0         0         0         0  \n",
       "41999         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## We will rank the features of the MNIST dataset using Fisher Linear Discriminant Ratio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# load dataset\n",
    "mnist_df = pd.read_csv(\"MNIST.csv\")\n",
    "\n",
    "# Import and validate the imported data \n",
    "print(f\"Shape of the Dataframe: {mnist_df.shape}\")\n",
    "print(f\"Head:\")\n",
    "display(mnist_df.head())\n",
    "print(f\"Tail:\")\n",
    "display(mnist_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLDR Output Shape: (784, 45)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(pixel0,pixel1)</th>\n",
       "      <th>(pixel0,pixel2)</th>\n",
       "      <th>(pixel0,pixel3)</th>\n",
       "      <th>(pixel0,pixel4)</th>\n",
       "      <th>(pixel0,pixel5)</th>\n",
       "      <th>(pixel0,pixel6)</th>\n",
       "      <th>(pixel0,pixel7)</th>\n",
       "      <th>(pixel0,pixel8)</th>\n",
       "      <th>(pixel0,pixel9)</th>\n",
       "      <th>(pixel1,pixel2)</th>\n",
       "      <th>...</th>\n",
       "      <th>(pixel5,pixel6)</th>\n",
       "      <th>(pixel5,pixel7)</th>\n",
       "      <th>(pixel5,pixel8)</th>\n",
       "      <th>(pixel5,pixel9)</th>\n",
       "      <th>(pixel6,pixel7)</th>\n",
       "      <th>(pixel6,pixel8)</th>\n",
       "      <th>(pixel6,pixel9)</th>\n",
       "      <th>(pixel7,pixel8)</th>\n",
       "      <th>(pixel7,pixel9)</th>\n",
       "      <th>(pixel8,pixel9)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     (pixel0,pixel1)  (pixel0,pixel2)  (pixel0,pixel3)  (pixel0,pixel4)  \\\n",
       "0                0.0              0.0              0.0              0.0   \n",
       "1                0.0              0.0              0.0              0.0   \n",
       "2                0.0              0.0              0.0              0.0   \n",
       "3                0.0              0.0              0.0              0.0   \n",
       "4                0.0              0.0              0.0              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "779              0.0              0.0              0.0              0.0   \n",
       "780              0.0              0.0              0.0              0.0   \n",
       "781              0.0              0.0              0.0              0.0   \n",
       "782              0.0              0.0              0.0              0.0   \n",
       "783              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     (pixel0,pixel5)  (pixel0,pixel6)  (pixel0,pixel7)  (pixel0,pixel8)  \\\n",
       "0                0.0              0.0         0.000000              0.0   \n",
       "1                0.0              0.0         0.000000              0.0   \n",
       "2                0.0              0.0         0.000000              0.0   \n",
       "3                0.0              0.0         0.000000              0.0   \n",
       "4                0.0              0.0         0.000000              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "779              0.0              0.0         0.000454              0.0   \n",
       "780              0.0              0.0         0.000000              0.0   \n",
       "781              0.0              0.0         0.000000              0.0   \n",
       "782              0.0              0.0         0.000000              0.0   \n",
       "783              0.0              0.0         0.000000              0.0   \n",
       "\n",
       "     (pixel0,pixel9)  (pixel1,pixel2)  ...  (pixel5,pixel6)  (pixel5,pixel7)  \\\n",
       "0                0.0              0.0  ...              0.0         0.000000   \n",
       "1                0.0              0.0  ...              0.0         0.000000   \n",
       "2                0.0              0.0  ...              0.0         0.000000   \n",
       "3                0.0              0.0  ...              0.0         0.000000   \n",
       "4                0.0              0.0  ...              0.0         0.000000   \n",
       "..               ...              ...  ...              ...              ...   \n",
       "779              0.0              0.0  ...              0.0         0.000454   \n",
       "780              0.0              0.0  ...              0.0         0.000000   \n",
       "781              0.0              0.0  ...              0.0         0.000000   \n",
       "782              0.0              0.0  ...              0.0         0.000000   \n",
       "783              0.0              0.0  ...              0.0         0.000000   \n",
       "\n",
       "     (pixel5,pixel8)  (pixel5,pixel9)  (pixel6,pixel7)  (pixel6,pixel8)  \\\n",
       "0                0.0              0.0         0.000000              0.0   \n",
       "1                0.0              0.0         0.000000              0.0   \n",
       "2                0.0              0.0         0.000000              0.0   \n",
       "3                0.0              0.0         0.000000              0.0   \n",
       "4                0.0              0.0         0.000000              0.0   \n",
       "..               ...              ...              ...              ...   \n",
       "779              0.0              0.0         0.000454              0.0   \n",
       "780              0.0              0.0         0.000000              0.0   \n",
       "781              0.0              0.0         0.000000              0.0   \n",
       "782              0.0              0.0         0.000000              0.0   \n",
       "783              0.0              0.0         0.000000              0.0   \n",
       "\n",
       "     (pixel6,pixel9)  (pixel7,pixel8)  (pixel7,pixel9)  (pixel8,pixel9)  \n",
       "0                0.0         0.000000         0.000000              0.0  \n",
       "1                0.0         0.000000         0.000000              0.0  \n",
       "2                0.0         0.000000         0.000000              0.0  \n",
       "3                0.0         0.000000         0.000000              0.0  \n",
       "4                0.0         0.000000         0.000000              0.0  \n",
       "..               ...              ...              ...              ...  \n",
       "779              0.0         0.000454         0.000454              0.0  \n",
       "780              0.0         0.000000         0.000000              0.0  \n",
       "781              0.0         0.000000         0.000000              0.0  \n",
       "782              0.0         0.000000         0.000000              0.0  \n",
       "783              0.0         0.000000         0.000000              0.0  \n",
       "\n",
       "[784 rows x 45 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We Will first create a function to calculate the FLDR for each feature comparison\n",
    "def feature_comparison_fldr(dataframe, class_column_index):\n",
    "    \n",
    "    groups = dataframe.groupby(class_column_index)\n",
    "    n_groups = groups.ngroups\n",
    "    n_features = dataframe.shape[1] - 1\n",
    "    col_means, col_var = groups.mean(), groups.var() # Get 60 columns/features means and variance\n",
    "    fdr_matrix = np.zeros((n_features, int(n_groups*(n_groups-1)/2))) # Initialize matrix to store FDR values\n",
    "    column_names = [] # Create list for column names\n",
    "    \n",
    "    k = 0 # Math\n",
    "    for i in range(n_groups):\n",
    "        for j in range(i+1, n_groups):\n",
    "            mean_dif = col_means.iloc[i,:] - col_means.iloc[j,:]\n",
    "            numerator = mean_dif**2\n",
    "            denominator = col_var.iloc[i,:] + col_var.iloc[j,:] + 1e-7 # Adjust for float point\n",
    "            fdr = numerator / denominator\n",
    "            fdr_matrix[:, k] = fdr\n",
    "            column_names.append(f'({col_means.columns[i]},{col_means.columns[j]})') # This will display the featrures names on top\n",
    "            k += 1\n",
    "    # Create dataframe with FDR values and column names\n",
    "    fdr_df = pd.DataFrame(fdr_matrix, columns=column_names)\n",
    "    return fdr_df\n",
    "    \n",
    "fdr_df = feature_comparison_fldr(mnist_df, 'label')\n",
    "print(f'FLDR Output Shape: {fdr_df.shape}')\n",
    "display(fdr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406    1.619782\n",
       "434    1.449687\n",
       "378    1.386104\n",
       "350    0.992946\n",
       "433    0.897859\n",
       "462    0.893507\n",
       "409    0.882876\n",
       "461    0.873374\n",
       "405    0.779696\n",
       "437    0.766129\n",
       "489    0.746681\n",
       "436    0.746238\n",
       "381    0.687322\n",
       "351    0.685433\n",
       "377    0.683459\n",
       "379    0.681190\n",
       "597    0.657605\n",
       "542    0.654889\n",
       "408    0.654405\n",
       "407    0.650710\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will calculate the FLDA and rank each feature based on its score\n",
    "def rank_feature_fldr(dataframe, class_column_index):\n",
    "    groups = dataframe.groupby(class_column_index)\n",
    "    n_groups = groups.ngroups\n",
    "    n_features = dataframe.shape[1] - 1\n",
    "    col_means, col_var = groups.mean(), groups.var()\n",
    "    fdr_matrix = np.zeros((n_features, int(n_groups * (n_groups - 1) / 2)))\n",
    "    column_names = []\n",
    "\n",
    "    k = 0\n",
    "    for i in range(n_groups):\n",
    "        for j in range(i + 1, n_groups):\n",
    "            mean_dif = col_means.iloc[i, :] - col_means.iloc[j, :]\n",
    "            numerator = mean_dif ** 2\n",
    "            denominator = col_var.iloc[i, :] + col_var.iloc[j, :] + 1e-10 # The 1e-10 is added to avoid division by zero\n",
    "            fdr = numerator / denominator\n",
    "            fdr_matrix[:, k] = fdr\n",
    "            column_names.append(f'({col_means.columns[i]},{col_means.columns[j]})')\n",
    "            k += 1\n",
    "\n",
    "    fdr_df = pd.DataFrame(fdr_matrix, columns=column_names)\n",
    "    average_fdr = fdr_df.mean(axis=1) # Compute the average FDR for each feature\n",
    "    sorted_features = average_fdr.sort_values(ascending=False)\n",
    "    \n",
    "    return sorted_features\n",
    "\n",
    "ranked_features_df = rank_feature_fldr(mnist_df, 'label')\n",
    "display(ranked_features_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the top 30 features\n",
    "n_features = 50\n",
    "top_features = ranked_features_df.head(n_features).index\n",
    "top_FLDR_values = ranked_features_df.head(n_features).values\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Adjust the figure size to avoid overcrowding\n",
    "plt.bar(range(n_features), top_FLDR_values)\n",
    "plt.xticks(range(n_features), top_features, rotation=90)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Fisher Discriminant Ratio')\n",
    "plt.title('Top 50 Features: Fisher Discriminant Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.<b> For part 2, you will create and fit a decision tree classifier (using the scikit learn built in function) offering a new avenue for feature ranking.<br>\n",
    "    (a) Create an algorithm that will accept your dataset and fit a decision tree classifier\n",
    "to it. <br>(b) Using your algorithm in part 2.1, run a 5 fold cross validation and score the\n",
    "model based on its ability to classify accurately.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree trained. Cross Validating in progress...\n",
      "[0.85797619 0.8527381  0.85714286 0.85321429 0.85797619]\n",
      "Mean Accuracy  85.58%\n",
      "Standard Deviation:  0.23%\n"
     ]
    }
   ],
   "source": [
    "## Type the response for part 2 (Decision Tree) here ##\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Prepare the dataframe\n",
    "labels = mnist_df['label']\n",
    "mnist_features = mnist_df.drop(columns=['label'])\n",
    "\n",
    "# Now create the decision tree and fit/perform the five fold cross evaluation\n",
    "# I did not necessarily need to build functions, but I did in case I make a decision tree class in the future.\n",
    "def fit_dec_tree(dataframe, target_lab):\n",
    "    dec_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "    dec_tree_classifier.fit(dataframe, target_lab) # We can set max leafs to tune complexity of the tree if needed\n",
    "    return dec_tree_classifier\n",
    "\n",
    "def cross_validate(decision_tree, dataframe, target_lab):\n",
    "    scores = cross_val_score(decision_tree, dataframe, target_lab, cv=5) # 5 fold cross validation\n",
    "    return scores\n",
    "\n",
    "dec_tree = fit_dec_tree(mnist_features, labels)\n",
    "print(\"Decision tree trained. Cross Validating in progress...\")\n",
    "scores = cross_validate(dec_tree, mnist_features, labels)\n",
    "print(scores) \n",
    "print(f\"Mean Accuracy {scores.mean()*100: .2f}%\")\n",
    "print(f\"Standard Deviation: {scores.std()*100: .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (c) From the model you fit, extract the feature importances.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAHNCAYAAADc5aBuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAA9hAAAPYQGoP6dpAABS2klEQVR4nO3deXQUVf7+8aezQ1gNmhAJIYAKiIoT/GLQGBQMAio66oAjIgpqBAch48ywqCAzCio6DMoyIogrouBOFCK4oEQdILjigqxKIoJKWDQB8vn9wUn/aLqzdHUWoN6vc/qcdPW9XffW/qSqqzxmZgIAAAAAlwmr6wYAAAAAQF0gDAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFciDAE4Jnk8niq93nnnnRpvS6tWrQKOOysry6/s7t27NWLECCUmJiomJkadOnXSc889V6XxjB8/vtx+PvLII9XdLUnSihUrNH78eP3666818v11pVu3bt5pFxYWpoYNG6pt27a66qqrtGDBApWWltbo+MvmZbA8Ho/Gjx9f/Q2qZJxHyroGAMGKqOsGAEBNyMvL83n/z3/+U2+//baWLVvmM7xDhw610p5zzjlHkydP9hkWHx/vV+6Pf/yj/ve//2nSpEk6+eST9eyzz+rqq69WaWmp/vznP1dpXG+++aYaN27sMywlJcV54yuwYsUK3X333Ro0aJCaNGlSI+OoK61bt9YzzzwjSdqzZ482bNigl19+WVdddZXS09P12muv+U3n6jJkyBBddNFFQdfLy8tTixYtaqBFFY/zUHW9rgFAMAhDAI5JZ599ts/7448/XmFhYX7Da0uTJk0qHXdOTo5yc3O9AUiSzj//fG3atEl/+9vf1K9fP4WHh1c6rtTUVDVr1qxa2l1XfvvtN8XExDg6O1Jd6tWr5zfPhgwZoscff1w33HCDbrrpJs2fP79Gxt2iRQtHoaYulm+n69revXtVv379mmwaAFSKy+QAuNbPP/+soUOH6sQTT1RUVJRat26tsWPHqri42Kecx+PRrbfeqv/+9786+eSTFR0drQ4dOlT58rWqeumll9SgQQNdddVVPsOvv/56bd26VR999FHI4zAzTZ8+XZ06dVK9evXUtGlTXXnllVq/fr1PudzcXPXt21ctWrRQTEyM2rZtq5tvvlnbt2/3lhk/frz+9re/STp45unwy6HKu2SrVatWGjRokPf93Llz5fF4tGTJEt1www06/vjjVb9+fe98mD9/vtLS0hQbG6sGDRqoZ8+eys/P9/nO9evXq3///kpMTFR0dLTi4+PVvXt3rVmzJuRpdrjrr79evXv31gsvvKBNmzZ5h1d12koHz951795djRs3Vv369dW+fXtNnDjR+3mgy+SWLVumbt26KS4uTvXq1VPLli11xRVXaO/evd4ygab5559/rr59+6pp06beSy+feOIJnzLvvPOOPB6P5s2bp7FjxyoxMVGNGjVSjx499PXXX4cyuSQdvOywY8eOeu+999S1a1fVr19fN9xwgySpqKhIt99+u1JSUhQVFaUTTzxRI0aM0J49e3y+I5jpCwBVRRgC4Eq///67zj//fD355JPKzs7WokWLNGDAAN1///364x//6Ff+1Vdf1dSpUzVhwgQtWLBAycnJuvrqq7VgwYIqje+9995Tw4YNFRkZqQ4dOujBBx/UgQMHfMp8/vnnat++vSIifE/an3766d7Pq+LAgQPav3+/93XoeG6++WaNGDFCPXr00Msvv6zp06friy++UNeuXfXjjz96y3333XdKS0vTjBkztGTJEt1111366KOPdO6552rfvn2SDp4l+ctf/iJJevHFF5WXl6e8vDz94Q9/qFI7D3fDDTcoMjJSTz31lBYsWKDIyEjde++9uvrqq9WhQwc9//zzeuqpp7Rr1y6lp6fryy+/9Nbt3bu3Vq1apfvvv1+5ubmaMWOGzjzzTJ/fMpWFrrlz5zpq36EuvfRSmZmWL1/uHVbVaTt79mz17t1bpaWlmjlzpl577TUNHz5c33//fbnj27hxo/r06aOoqCjNmTNHb775piZNmqTY2FiVlJSUW+/rr79W165d9cUXX2jq1Kl68cUX1aFDBw0aNEj333+/X/kxY8Zo06ZNeuyxx/Too4/q22+/1SWXXOK3rDpRUFCgAQMG6M9//rNycnI0dOhQ7d27VxkZGXriiSc0fPhwvfHGG/rHP/6huXPneqdxmapOXwAIigGAC1x33XUWGxvrfT9z5kyTZM8//7xPufvuu88k2ZIlS7zDJFm9evWssLDQO2z//v3Wrl07a9u2baXjHjp0qM2ZM8feffdde/nll+2aa64xSTZgwACfcieddJL17NnTr/7WrVtNkt17770VjmfcuHEmye914oknmplZXl6eSbIHH3zQp96WLVusXr169ve//z3g95aWltq+ffts06ZNJsleeeUV72cPPPCASbINGzb41ZNk48aN8xuenJxs1113nff9448/bpJs4MCBPuU2b95sERER9pe//MVn+K5duywhIcH+9Kc/mZnZ9u3bTZJNmTKl3GljZvbEE09YeHi4PfHEExWWMzPLyMiwU089tdzP33jjDZNk9913n5lVfdru2rXLGjVqZOeee66VlpaW+/1l87LMggULTJKtWbOmwnYfPs379+9v0dHRtnnzZp9yvXr1svr169uvv/5qZmZvv/22SbLevXv7lHv++edNkuXl5VU43kMdvq6ZHZyekmzp0qU+wydOnGhhYWH2v//9z2d4WX9zcnLMzPmyCwCV4cwQAFdatmyZYmNjdeWVV/oML7t8a+nSpT7Du3fv7nPDg/DwcPXr10/r1q2r8D/6kjRt2jRdf/31Ou+889S3b189/fTTuvXWW/X000/7Xe5V0W9kqvr7mbfeekv/+9//vK+cnBxJ0uuvvy6Px6MBAwb4nDlKSEjQGWec4XO3r23btikrK0tJSUmKiIhQZGSkkpOTJUlr166tUjuCdcUVV/i8X7x4sfbv36+BAwf6tDcmJkYZGRne9h533HFq06aNHnjgAT300EPKz88PeLe3su8ZOHBgyG21Q85YSFWftitWrFBRUZGGDh0a1O+hOnXqpKioKN1000164oknqnxp2LJly9S9e3clJSX5DB80aJD27t3rd/ODSy+91Od92VnJQy8HdKpp06a64IILfIa9/vrr6tixozp16uQz3Xr27OlzyWUwyy4ABIMbKABwpR07dighIcHvgPSEE05QRESEduzY4TM8ISHB7zvKhu3YsSPoH7sPGDBAjzzyiD788EOdeeaZkqS4uDi/8UoHf9skHTzor4ozzjgj4A0UfvzxR5lZwLvYSQfvniZJpaWlyszM1NatW3XnnXfqtNNOU2xsrEpLS3X22Wfrt99+q1I7gtW8eXO/9krSWWedFbB8WNjB/+d5PB4tXbpUEyZM0P3336+//vWvOu6443TNNdfonnvuUcOGDau9rWXhIDEx0dvWqkzbn376SZKCXl7atGmjt956S/fff7+GDRumPXv2qHXr1ho+fLhuu+22cuvt2LHDb7oe2u7Dl7e4uDif99HR0ZJULfM8UDt+/PFHrVu3TpGRkQHrlP1GrarTFwCCRRgC4EpxcXH66KOPZGY+gWjbtm3av3+/X5goLCz0+46yYYcfQFZF2ZmFsgN6STrttNM0b9487d+/3+d3Q5999pkkqWPHjkGP51DNmjWTx+PR8uXLvQe5hyob9vnnn+uTTz7R3Llzdd1113k/X7duXVDji46O9rsZheR/AF7m8GBaNg/KfqNVkeTkZM2ePVuS9M033+j555/X+PHjVVJSopkzZwbV7qp49dVX5fF4dN5553nbWpVpe/zxx0tSpWcTA0lPT1d6eroOHDiglStX6uGHH9aIESMUHx+v/v37B6wTFxengoICv+Fbt271tru2BDoT1qxZM9WrV09z5swJWKesfVWdvgAQLMIQAFfq3r27nn/+eb388su6/PLLvcOffPJJ7+eHWrp0qX788Ufvf6YPHDig+fPnq02bNo5ugVw2nkNvP3z55Zdr1qxZWrhwofr16+cd/sQTTygxMVFdunQJejyHuvjiizVp0iT98MMP+tOf/lRuubKD1sMPMP/73//6la3ozEGrVq306aef+gxbtmyZdu/eXaX29uzZUxEREfruu+/8LqGryMknn6w77rhDCxcu1OrVq6tcr6oef/xxvfHGG/rzn/+sli1bSqr6tO3atasaN26smTNnqn///o5uHR4eHq4uXbqoXbt2euaZZ7R69epyw1D37t310ksvaevWrd6zQdLB5a9+/fp1dqv5MhdffLHuvfdexcXFVfgsrKpOXwAIFmEIgCsNHDhQ06ZN03XXXaeNGzfqtNNO0/vvv697771XvXv3Vo8ePXzKN2vWTBdccIHuvPNOxcbGavr06frqq68qvb32s88+qxdffFF9+vRRcnKyfv31V73wwgt67rnnNGjQIJ1xxhnesr169dKFF16oW265RUVFRWrbtq3mzZunN998U08//XSVnjFUkXPOOUc33XSTrr/+eq1cuVLnnXeeYmNjVVBQoPfff1+nnXaabrnlFrVr105t2rTRqFGjZGY67rjj9Nprryk3N9fvO0877TRJ0n/+8x9dd911ioyM1CmnnKKGDRvq2muv1Z133qm77rpLGRkZ+vLLL/XII49U+UGlrVq10oQJEzR27FitX79eF110kZo2baoff/xRH3/8sWJjY3X33Xfr008/1a233qqrrrpKJ510kqKiorRs2TJ9+umnGjVqlPf7nnzySd1www2aM2dOlX439Ntvv+nDDz/0/r1+/Xq9/PLLev3115WRkeFzxqmq07ZBgwZ68MEHNWTIEPXo0UM33nij4uPjtW7dOn3yySd65JFHArZl5syZWrZsmfr06aOWLVvq999/955NOXxZPdS4ceP0+uuv6/zzz9ddd92l4447Ts8884wWLVqk+++/v8YeGltVI0aM0MKFC3Xeeedp5MiROv3001VaWqrNmzdryZIl+utf/6ouXbpUefoCQNDq8OYNAFBrAt3haseOHZaVlWXNmze3iIgIS05OttGjR9vvv//uU06SDRs2zKZPn25t2rSxyMhIa9eunT3zzDOVjjcvL8+6d+9uCQkJFhkZafXr17ezzjrLpk+fbgcOHPArv2vXLhs+fLglJCRYVFSUnX766TZv3rwq9bHsDmQ//fRTheXmzJljXbp0sdjYWKtXr561adPGBg4caCtXrvSW+fLLL+3CCy+0hg0bWtOmTe2qq66yzZs3B7xD3OjRoy0xMdHCwsJMkr399ttmZlZcXGx///vfLSkpyerVq2cZGRm2Zs2acu8md/gdxcq8/PLLdv7551ujRo0sOjrakpOT7corr7S33nrLzMx+/PFHGzRokLVr185iY2OtQYMGdvrpp9u///1v279/v994Hn/88UqnZdndz8pesbGx1rp1a7vyyivthRdeCDjvqjptzcxycnIsIyPDYmNjrX79+tahQwfvnenM/O8ml5eXZ5dffrklJydbdHS0xcXFWUZGhr366qs+3xto/nz22Wd2ySWXWOPGjS0qKsrOOOMMv2lQdje5F154wWf4hg0bqjzNypR3N7ny7s63e/duu+OOO+yUU06xqKgoa9y4sZ122mk2cuRInzs4mlV9+gJAVXnMDrslDgDAh8fj0bBhw8r9rz0AADg6cWttAAAAAK5EGAIAAADgStxAAQAqwdXEAAAcmzgzBAAAAMCVCEMAAAAAXIkwBAAAAMCVjpnfDJWWlmrr1q1q2LChoyd6AwAAADg2mJl27dqlxMREhYWVf/7nmAlDW7duVVJSUl03AwAAAMARYsuWLWrRokW5nx8zYahhw4aSDna4UaNGddwaAAAAAHWlqKhISUlJ3oxQnmMmDJVdGteoUSPCEAAAAIBKfz7DDRQAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuJKjMDR9+nSlpKQoJiZGqampWr58eYXl3333XaWmpiomJkatW7fWzJkz/cr8+uuvGjZsmJo3b66YmBi1b99eOTk5TpoHAAAAAJUKOgzNnz9fI0aM0NixY5Wfn6/09HT16tVLmzdvDlh+w4YN6t27t9LT05Wfn68xY8Zo+PDhWrhwobdMSUmJLrzwQm3cuFELFizQ119/rVmzZunEE0903jMAAAAAqIDHzCyYCl26dNEf/vAHzZgxwzusffv2uuyyyzRx4kS/8v/4xz/06quvau3atd5hWVlZ+uSTT5SXlydJmjlzph544AF99dVXioyMdNSRoqIiNW7cWDt37uQ5QwAAAICLVTUbBHVmqKSkRKtWrVJmZqbP8MzMTK1YsSJgnby8PL/yPXv21MqVK7Vv3z5J0quvvqq0tDQNGzZM8fHx6tixo+69914dOHCg3LYUFxerqKjI5wUAAAAAVRVUGNq+fbsOHDig+Ph4n+Hx8fEqLCwMWKewsDBg+f3792v79u2SpPXr12vBggU6cOCAcnJydMcdd+jBBx/UPffcU25bJk6cqMaNG3tfSUlJwXQFAAAAgMs5uoGCx+PxeW9mfsMqK3/o8NLSUp1wwgl69NFHlZqaqv79+2vs2LE+l+IdbvTo0dq5c6f3tWXLFiddAQAAAOBSEcEUbtasmcLDw/3OAm3bts3v7E+ZhISEgOUjIiIUFxcnSWrevLkiIyMVHh7uLdO+fXsVFhaqpKREUVFRft8bHR2t6OjoYJoPAAAAAF5BnRmKiopSamqqcnNzfYbn5uaqa9euAeukpaX5lV+yZIk6d+7svVnCOeeco3Xr1qm0tNRb5ptvvlHz5s0DBiEAAAAACFXQl8llZ2frscce05w5c7R27VqNHDlSmzdvVlZWlqSDl68NHDjQWz4rK0ubNm1Sdna21q5dqzlz5mj27Nm6/fbbvWVuueUW7dixQ7fddpu++eYbLVq0SPfee6+GDRtWDV0EAAAAAH9BXSYnSf369dOOHTs0YcIEFRQUqGPHjsrJyVFycrIkqaCgwOeZQykpKcrJydHIkSM1bdo0JSYmaurUqbriiiu8ZZKSkrRkyRKNHDlSp59+uk488UTddttt+sc//lENXQQAAAAAf0E/Z+hIdaQ9Z6jVqEVVLrtxUp8abAkAAADgLjXynCEAAAAAOFYQhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCs5CkPTp09XSkqKYmJilJqaquXLl1dY/t1331VqaqpiYmLUunVrzZw50+fzuXPnyuPx+L1+//13J80DAAAAgEoFHYbmz5+vESNGaOzYscrPz1d6erp69eqlzZs3Byy/YcMG9e7dW+np6crPz9eYMWM0fPhwLVy40Kdco0aNVFBQ4POKiYlx1isAAAAAqEREsBUeeughDR48WEOGDJEkTZkyRYsXL9aMGTM0ceJEv/IzZ85Uy5YtNWXKFElS+/bttXLlSk2ePFlXXHGFt5zH41FCQoLDbgAAAABAcII6M1RSUqJVq1YpMzPTZ3hmZqZWrFgRsE5eXp5f+Z49e2rlypXat2+fd9ju3buVnJysFi1a6OKLL1Z+fn6FbSkuLlZRUZHPCwAAAACqKqgwtH37dh04cEDx8fE+w+Pj41VYWBiwTmFhYcDy+/fv1/bt2yVJ7dq109y5c/Xqq69q3rx5iomJ0TnnnKNvv/223LZMnDhRjRs39r6SkpKC6QoAAAAAl3N0AwWPx+Pz3sz8hlVW/tDhZ599tgYMGKAzzjhD6enpev7553XyySfr4YcfLvc7R48erZ07d3pfW7ZscdIVAAAAAC4V1G+GmjVrpvDwcL+zQNu2bfM7+1MmISEhYPmIiAjFxcUFrBMWFqazzjqrwjND0dHRio6ODqb5AAAAAOAV1JmhqKgopaamKjc312d4bm6uunbtGrBOWlqaX/klS5aoc+fOioyMDFjHzLRmzRo1b948mOYBAAAAQJUFfZlcdna2HnvsMc2ZM0dr167VyJEjtXnzZmVlZUk6ePnawIEDveWzsrK0adMmZWdna+3atZozZ45mz56t22+/3Vvm7rvv1uLFi7V+/XqtWbNGgwcP1po1a7zfCQAAAADVLehba/fr1087duzQhAkTVFBQoI4dOyonJ0fJycmSpIKCAp9nDqWkpCgnJ0cjR47UtGnTlJiYqKlTp/rcVvvXX3/VTTfdpMLCQjVu3Fhnnnmm3nvvPf3f//1fNXQRAAAAAPx5rOxuBke5oqIiNW7cWDt37lSjRo3qujlqNWpRlctunNSnBlsCAAAAuEtVs4Gju8kBAAAAwNGOMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlSLqugHw1WrUoiqX3TipTw22BAAAADi2cWYIAAAAgCsRhgAAAAC4EmEIAAAAgCvxm6FjRDC/NZL4vREAAADAmSEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAArkQYAgAAAOBKhCEAAAAAruQoDE2fPl0pKSmKiYlRamqqli9fXmH5d999V6mpqYqJiVHr1q01c+bMcss+99xz8ng8uuyyy5w0DQAAAACqJOgwNH/+fI0YMUJjx45Vfn6+0tPT1atXL23evDlg+Q0bNqh3795KT09Xfn6+xowZo+HDh2vhwoV+ZTdt2qTbb79d6enpwfcEAAAAAIIQEWyFhx56SIMHD9aQIUMkSVOmTNHixYs1Y8YMTZw40a/8zJkz1bJlS02ZMkWS1L59e61cuVKTJ0/WFVdc4S134MABXXPNNbr77ru1fPly/frrrxW2o7i4WMXFxd73RUVFwXYFklqNWhRU+Y2T+tRQSwAAAIDaFVQYKikp0apVqzRq1Cif4ZmZmVqxYkXAOnl5ecrMzPQZ1rNnT82ePVv79u1TZGSkJGnChAk6/vjjNXjw4Eovu5OkiRMn6u677w6m+ahmwQQpQhQAAACONEFdJrd9+3YdOHBA8fHxPsPj4+NVWFgYsE5hYWHA8vv379f27dslSR988IFmz56tWbNmVbkto0eP1s6dO72vLVu2BNMVAAAAAC4X9GVykuTxeHzem5nfsMrKlw3ftWuXBgwYoFmzZqlZs2ZVbkN0dLSio6ODaDUAAAAA/H9BhaFmzZopPDzc7yzQtm3b/M7+lElISAhYPiIiQnFxcfriiy+0ceNGXXLJJd7PS0tLDzYuIkJff/212rRpE0wzAQAAAKBSQV0mFxUVpdTUVOXm5voMz83NVdeuXQPWSUtL8yu/ZMkSde7cWZGRkWrXrp0+++wzrVmzxvu69NJLdf7552vNmjVKSkoKsksAAAAAULmgL5PLzs7Wtddeq86dOystLU2PPvqoNm/erKysLEkHf8vzww8/6Mknn5QkZWVl6ZFHHlF2drZuvPFG5eXlafbs2Zo3b54kKSYmRh07dvQZR5MmTSTJbzgAAAAAVJegw1C/fv20Y8cOTZgwQQUFBerYsaNycnKUnJwsSSooKPB55lBKSopycnI0cuRITZs2TYmJiZo6darPbbUBAAAAoLY5uoHC0KFDNXTo0ICfzZ07129YRkaGVq9eXeXvD/QdAAAAAFCdgvrNEAAAAAAcKwhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlSLqugFwn1ajFlW57MZJfWqwJQAAAHAzzgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABX4gYKOGpw4wUAAABUJ84MAQAAAHAlwhAAAAAAVyIMAQAAAHAlwhAAAAAAVyIMAQAAAHAlwhAAAAAAVyIMAQAAAHAlwhAAAAAAVyIMAQAAAHAlwhAAAAAAVyIMAQAAAHAlwhAAAAAAV3IUhqZPn66UlBTFxMQoNTVVy5cvr7D8u+++q9TUVMXExKh169aaOXOmz+cvvviiOnfurCZNmig2NladOnXSU0895aRpAAAAAFAlQYeh+fPna8SIERo7dqzy8/OVnp6uXr16afPmzQHLb9iwQb1791Z6erry8/M1ZswYDR8+XAsXLvSWOe644zR27Fjl5eXp008/1fXXX6/rr79eixcvdt4zAAAAAKhARLAVHnroIQ0ePFhDhgyRJE2ZMkWLFy/WjBkzNHHiRL/yM2fOVMuWLTVlyhRJUvv27bVy5UpNnjxZV1xxhSSpW7duPnVuu+02PfHEE3r//ffVs2fPgO0oLi5WcXGx931RUVGwXQEAAADgYkGdGSopKdGqVauUmZnpMzwzM1MrVqwIWCcvL8+vfM+ePbVy5Urt27fPr7yZaenSpfr666913nnnlduWiRMnqnHjxt5XUlJSMF0BAAAA4HJBhaHt27frwIEDio+P9xkeHx+vwsLCgHUKCwsDlt+/f7+2b9/uHbZz5041aNBAUVFR6tOnjx5++GFdeOGF5bZl9OjR2rlzp/e1ZcuWYLoCAAAAwOWCvkxOkjwej897M/MbVln5w4c3bNhQa9as0e7du7V06VJlZ2erdevWfpfQlYmOjlZ0dLST5gMAAABAcGGoWbNmCg8P9zsLtG3bNr+zP2USEhIClo+IiFBcXJx3WFhYmNq2bStJ6tSpk9auXauJEyeWG4YAAAAAIBRBhaGoqCilpqYqNzdXl19+uXd4bm6u+vbtG7BOWlqaXnvtNZ9hS5YsUefOnRUZGVnuuMzM5wYJgFOtRi2qctmNk/rUYEsAAABwJAn6Mrns7Gxde+216ty5s9LS0vToo49q8+bNysrKknTwtzw//PCDnnzySUlSVlaWHnnkEWVnZ+vGG29UXl6eZs+erXnz5nm/c+LEiercubPatGmjkpIS5eTk6Mknn9SMGTOqqZsAAAAA4CvoMNSvXz/t2LFDEyZMUEFBgTp27KicnBwlJydLkgoKCnyeOZSSkqKcnByNHDlS06ZNU2JioqZOneq9rbYk7dmzR0OHDtX333+vevXqqV27dnr66afVr1+/augiAAAAAPhzdAOFoUOHaujQoQE/mzt3rt+wjIwMrV69utzv+9e//qV//etfTpoCAAAAAI4EdWttAAAAADhWEIYAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArRdR1A4AjVatRi6pcduOkPjXYEgAAANQEzgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcCXCEAAAAABXIgwBAAAAcKWIum4AcKxpNWpRUOU3TupTQy0BAABARTgzBAAAAMCVHIWh6dOnKyUlRTExMUpNTdXy5csrLP/uu+8qNTVVMTExat26tWbOnOnz+axZs5Senq6mTZuqadOm6tGjhz7++GMnTQMAAACAKgk6DM2fP18jRozQ2LFjlZ+fr/T0dPXq1UubN28OWH7Dhg3q3bu30tPTlZ+frzFjxmj48OFauHCht8w777yjq6++Wm+//bby8vLUsmVLZWZm6ocffnDeMwAAAACoQNBh6KGHHtLgwYM1ZMgQtW/fXlOmTFFSUpJmzJgRsPzMmTPVsmVLTZkyRe3bt9eQIUN0ww03aPLkyd4yzzzzjIYOHapOnTqpXbt2mjVrlkpLS7V06VLnPQMAAACACgQVhkpKSrRq1SplZmb6DM/MzNSKFSsC1snLy/Mr37NnT61cuVL79u0LWGfv3r3at2+fjjvuuHLbUlxcrKKiIp8XAAAAAFRVUGFo+/btOnDggOLj432Gx8fHq7CwMGCdwsLCgOX379+v7du3B6wzatQonXjiierRo0e5bZk4caIaN27sfSUlJQXTFQAAAAAu5+gGCh6Px+e9mfkNq6x8oOGSdP/992vevHl68cUXFRMTU+53jh49Wjt37vS+tmzZEkwXAAAAALhcUM8ZatasmcLDw/3OAm3bts3v7E+ZhISEgOUjIiIUFxfnM3zy5Mm699579dZbb+n000+vsC3R0dGKjo4OpvkAAAAA4BXUmaGoqCilpqYqNzfXZ3hubq66du0asE5aWppf+SVLlqhz586KjIz0DnvggQf0z3/+U2+++aY6d+4cTLMAAAAAIGhBXyaXnZ2txx57THPmzNHatWs1cuRIbd68WVlZWZIOXr42cOBAb/msrCxt2rRJ2dnZWrt2rebMmaPZs2fr9ttv95a5//77dccdd2jOnDlq1aqVCgsLVVhYqN27d1dDFwEAAADAX1CXyUlSv379tGPHDk2YMEEFBQXq2LGjcnJylJycLEkqKCjweeZQSkqKcnJyNHLkSE2bNk2JiYmaOnWqrrjiCm+Z6dOnq6SkRFdeeaXPuMaNG6fx48c77BpwdGk1alFQ5TdO6lNDLQEAAHCHoMOQJA0dOlRDhw4N+NncuXP9hmVkZGj16tXlft/GjRudNAMAAAAAHHN0NzkAAAAAONoRhgAAAAC4EmEIAAAAgCsRhgAAAAC4EmEIAAAAgCsRhgAAAAC4kqNbawM4sgTzjCKeTwQAAHAQZ4YAAAAAuBJhCAAAAIArcZkc4GJcXgcAANyMM0MAAAAAXIkwBAAAAMCVCEMAAAAAXIkwBAAAAMCVCEMAAAAAXIkwBAAAAMCVCEMAAAAAXInnDAEIGs8nAgAAxwLODAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFciDAEAAABwJcIQAAAAAFfioasAag0PawUAAEcSzgwBAAAAcCXCEAAAAABX4jI5AEc8Lq8DAAA1gTNDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlSLqugEAUFNajVoUVPmNk/rUUEsAAMCRiDNDAAAAAFyJM0MAcBjOKAEA4A6cGQIAAADgSoQhAAAAAK7k6DK56dOn64EHHlBBQYFOPfVUTZkyRenp6eWWf/fdd5Wdna0vvvhCiYmJ+vvf/66srCzv51988YXuuusurVq1Sps2bdK///1vjRgxwknTAKBOBXOJHZfXAQBQt4I+MzR//nyNGDFCY8eOVX5+vtLT09WrVy9t3rw5YPkNGzaod+/eSk9PV35+vsaMGaPhw4dr4cKF3jJ79+5V69atNWnSJCUkJDjvDQAAAABUUdBh6KGHHtLgwYM1ZMgQtW/fXlOmTFFSUpJmzJgRsPzMmTPVsmVLTZkyRe3bt9eQIUN0ww03aPLkyd4yZ511lh544AH1799f0dHRznsDAAAAAFUU1GVyJSUlWrVqlUaNGuUzPDMzUytWrAhYJy8vT5mZmT7DevbsqdmzZ2vfvn2KjIwMsskHFRcXq7i42Pu+qKjI0fcAwJGAy+sAAKh9QZ0Z2r59uw4cOKD4+Hif4fHx8SosLAxYp7CwMGD5/fv3a/v27UE29/+bOHGiGjdu7H0lJSU5/i4AAAAA7uPoBgoej8fnvZn5DausfKDhwRg9erSys7O974uKighEAFyHM0oAADgXVBhq1qyZwsPD/c4Cbdu2ze/sT5mEhISA5SMiIhQXFxdkc/+/6Ohofl8EAAAAwLGgLpOLiopSamqqcnNzfYbn5uaqa9euAeukpaX5lV+yZIk6d+7s+PdCAAAAABCqoO8ml52drccee0xz5szR2rVrNXLkSG3evNn73KDRo0dr4MCB3vJZWVnatGmTsrOztXbtWs2ZM0ezZ8/W7bff7i1TUlKiNWvWaM2aNSopKdEPP/ygNWvWaN26ddXQRQAAAADwF/Rvhvr166cdO3ZowoQJKigoUMeOHZWTk6Pk5GRJUkFBgc8zh1JSUpSTk6ORI0dq2rRpSkxM1NSpU3XFFVd4y2zdulVnnnmm9/3kyZM1efJkZWRk6J133gmhewAAAAAQmKMbKAwdOlRDhw4N+NncuXP9hmVkZGj16tXlfl+rVq28N1UAAAAAgNoQ9GVyAAAAAHAsIAwBAAAAcCVHl8kBAI5uPJ8IAADCEAAgCMGEKIkgBQA4snGZHAAAAABXIgwBAAAAcCUukwMA1DgurwMAHIkIQwCAIxYhCgBQkwhDAIBjEnfMAwBUhjAEAMAhCFEA4B6EIQAAqoHTEEX4AoC6w93kAAAAALgSZ4YAADgKOb25BDelAID/jzNDAAAAAFyJMAQAAADAlQhDAAAAAFyJ3wwBAIAq4c53AI41hCEAAFCjCFEAjlRcJgcAAADAlTgzBAAAjkicUQJQ0whDAADgmEKIAlBVXCYHAAAAwJU4MwQAACDOKAFuRBgCAAAIQTAhSiJIAUcSLpMDAAAA4EqEIQAAAACuxGVyAAAAdYDL64C6RxgCAAA4yji92QM3iQB8EYYAAABQodoOX7VR79C6oZylI5ge3QhDAAAAwFGCEFW9CEMAAADAMY4QFRhhCAAAAEBAx3qI4tbaAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFzJURiaPn26UlJSFBMTo9TUVC1fvrzC8u+++65SU1MVExOj1q1ba+bMmX5lFi5cqA4dOig6OlodOnTQSy+95KRpAAAAAFAlQYeh+fPna8SIERo7dqzy8/OVnp6uXr16afPmzQHLb9iwQb1791Z6erry8/M1ZswYDR8+XAsXLvSWycvLU79+/XTttdfqk08+0bXXXqs//elP+uijj5z3DAAAAAAqEHQYeuihhzR48GANGTJE7du315QpU5SUlKQZM2YELD9z5ky1bNlSU6ZMUfv27TVkyBDdcMMNmjx5srfMlClTdOGFF2r06NFq166dRo8ere7du2vKlCmOOwYAAAAAFYkIpnBJSYlWrVqlUaNG+QzPzMzUihUrAtbJy8tTZmamz7CePXtq9uzZ2rdvnyIjI5WXl6eRI0f6lakoDBUXF6u4uNj7fufOnZKkoqKiYLpUY0qL91a57KFtro16h9Z1Wi/YutSrvC7zom7rHVqXeVE99Q6ty7yo23qH1mWa1m29Q+syL+q23qF1mRfVX6+ulbXFzCouaEH44YcfTJJ98MEHPsPvueceO/nkkwPWOemkk+yee+7xGfbBBx+YJNu6dauZmUVGRtozzzzjU+aZZ56xqKioctsybtw4k8SLFy9evHjx4sWLFy9eAV9btmypMN8EdWaojMfj8XlvZn7DKit/+PBgv3P06NHKzs72vi8tLdXPP/+suLi4CuvVpaKiIiUlJWnLli1q1KgR9eqo3tHU1qOl3tHU1mO93tHU1mO93tHU1mO93tHU1qOl3tHU1mO93tHW1tpiZtq1a5cSExMrLBdUGGrWrJnCw8NVWFjoM3zbtm2Kj48PWCchISFg+YiICMXFxVVYprzvlKTo6GhFR0f7DGvSpElVu1KnGjVq5GjBoV711quLcR7r9epinNQ7csZJvSNnnNQ7csZ5rNeri3FS78gZZyhtrQ2NGzeutExQN1CIiopSamqqcnNzfYbn5uaqa9euAeukpaX5lV+yZIk6d+6syMjICsuU950AAAAAEKqgL5PLzs7Wtddeq86dOystLU2PPvqoNm/erKysLEkHL1/74Ycf9OSTT0qSsrKy9Mgjjyg7O1s33nij8vLyNHv2bM2bN8/7nbfddpvOO+883Xffferbt69eeeUVvfXWW3r//ferqZsAAAAA4CvoMNSvXz/t2LFDEyZMUEFBgTp27KicnBwlJydLkgoKCnyeOZSSkqKcnByNHDlS06ZNU2JioqZOnaorrrjCW6Zr16567rnndMcdd+jOO+9UmzZtNH/+fHXp0qUaunjkiI6O1rhx4/wu76Ne7dari3Ee6/XqYpzUO3LGSb0jZ5zUO3LGeazXq4txUu/IGWcobT3SeMwqu98cAAAAABx7gn7oKgAAAAAcCwhDAAAAAFyJMAQAAADAlQhDAAAAAFyJMAQAAADAlQhDAAAAAFwp6OcMwbkLLrhAjz/+uPeZTOVZtmyZ3n//fRUUFCg8PFwpKSm69NJLddJJJ1V7mxYuXKhevXqpfv36jupXd1t/+eUXvfbaaxo4cKDfZ5988olWr16tbt26KSUlRV988YWmTZum0tJSXX755erZs6ejcVbEzPTWW29pxYoVKiwslMfjUXx8vM455xx1795dHo+nSt+zZs0affvtt2revLnOOeecKterSd9//71iYmLUrFkzSdLy5cs1c+ZMbd68WcnJyRo2bJjS0tJqtA3BTJdjeV4UFxcrLCxMkZGRkqTvvvtOc+bM8c6LwYMHKyUlpdz6TtfD2tzWSHWzDleXqm6/nQh1/gcr1O1+dajp9bC2+3gkTFM3qehY4WhyNG8TjymGavfKK68EfIWHh9sjjzzifX+4H3/80f7v//7PPB6PhYeHW1hYmKWmplpCQoKFh4fb3/72t6DakZ+fb88//7wtX77cSktLA5bxeDzWsGFDu/HGG+3DDz+s8ndXd1vLrFmzxsLCwvyGL1iwwMLDwy0uLs4aNmxob731ljVp0sR69OhhPXv2tPDwcHvmmWeqPJ6qTJvvv//eOnXqZOHh4XbGGWdYZmamXXjhhXbGGWdYeHi4/eEPf7Dvv//er97VV19tRUVFZma2a9cuy8zMNI/HY1FRUebxeKxz5872yy+/BOzjnj17qtyHQ5WWltqSJUts/PjxlpWVZbfccouNHz/ecnNzy+1fWlqa5eTkmJnZyy+/bGFhYXbppZfaP/7xD7v88sstMjLSXnvttQrHu3TpUrv77rstKyvLhg0bZpMnT7ZvvvkmYFmn08Ws9udFZX7++Wd74oknqlS2Ksva+eefbwsXLjQzs/fff9+io6Pt9NNPt379+tmZZ55p9evXtxUrVvjVc7oehrr+lpaW2vr1623fvn1mZlZcXGzPPfecPfHEE/bTTz8FrBPqOnzgwIFyh2/atKna2ul0+21m9vvvv1tJSYn3/bp162zMmDE2YMAAGzt2rK1fvz5gPafz3+zgNnPOnDne7/7888/tlltusZtvvtnefPPNgHWcbvfLBLPem4W2HjrdLobax8Odf/75tnHjxhoZn5PltIyT9SKUuk72NeWpbJpWpLxjBbPQ9qWHq8r222m9ULaJtX28UJlg9olHIsJQDfB4PBYWFmYej6fcV6CVuF+/fnbZZZfZL7/8Ynv37rVhw4bZwIEDzezgzicuLs6mTJkScJxOdzYej8cmTJhgZ555pnk8Hjv11FPt3//+t23fvr3CPjpt686dOyt8LV++POC0+cMf/mD/+te/zMxs3rx51qRJE5swYYL388mTJ1unTp2qddpceumldsEFF9jWrVv9Ptu6datdcMEF1rdvX7/PwsLC7McffzQzs9tvv91SUlJs1apVZmb22WefWfv27W3kyJF+9ZzuTJ0GhYYNG9qGDRvMzKxLly42adIkn88ffvhhO/PMMwOO08nBtNPpYlb786Iy5e2InS5rTZo0sXXr1pmZWUZGhl+b7rjjDjvnnHP86jldD0PZ1nz11VeWnJxsYWFh1rZtW1u/fr2lpqZabGys1a9f35o1axbwwNjpOrxz50676qqrLCYmxk444QS76667bP/+/d7PCwsLA84Lp+10uv02cx5qnM5/pwdTTrf7TkN0KOuh0+2i0z46DcNOx+d0OXW6XoRS1+m+xsk0dXqsYOZ8mXG6/XZaL5Tjmto+XqhMReH0aEAYqgEXXXSR9enTx7vxLxMREWFffPFFufUaNWpkn3/+uff97t27LTIy0nbu3GlmZk899ZSdcsopAeuGcgBeVm/lypV2yy23WJMmTSw6OtquuuoqW7JkSbW2texAorxXeQcasbGx3gP30tJSi4yMtE8//dT7+XfffWcNGjSo1mkTGxtra9asCfidZmarV6+22NjYgH0sG9+pp55q8+fP9/l80aJFdtJJJwWs52Rn6jQoNG7c2D755BMzMzvhhBO8f5dZt26d1a9fP+A4nRxMO50uZrU/L5zuiENZ1tauXWtmZvHx8X59XbduXcDl2+l6GMq2pm/fvnbppZfap59+aiNGjLAOHTpY3759raSkxIqLi61v3742YMCAgH10sg4PHz7cTj75ZHvhhRds1qxZlpycbH369LHi4mIzO3jg5vF4qq2dTrffZs5DjdP57/Rgyul232mIDmXdd7pddNpHp2HY6ficLqdO14tQ6jrd1ziZpk6PFcrqOllmnG6/Q9nuOz2uqe3jhVDC6dGAMFRDHnroIWvZsqXPZUaV7UyPP/54n8/37t1rYWFhtmPHDjM7uGJER0cHrBvKAfjhO/3ffvvNnnzySevWrZuFhYVZcnJytbW1UaNGdt9999k777wT8DVr1qyAK1RCQoKtXLnSzA6ejvV4PPb22297P//4448tISGhWqdNs2bNbNmyZQG/0+zgzr9Zs2YBx7dt2zbvdxw+zzdu3GgxMTEVtjOYnanToHDppZfaqFGjzMysZ8+e9p///Mfn81mzZpV7gOLkYNrpdCkrX9vzwsmO2OmydsEFF9j9999vZmZdu3b1u9xgwYIF1rJlS796TtfDULY1xx9/vOXn55vZwfnu8Xhs+fLl3s9XrFgRsK1O1+GWLVv6lNu+fbt16dLFMjMz7ffffy/3v9hO22nmbPtt5jzUOJ3/Tg+mnG73Q/knmNN1P5RQ46SPTsNwKPtSJ8up0/UilLpO9zVOpqnTYwWz6llmnB5HBVOvuo5rauN4IZRwejQgDNWgNWvWWIcOHeymm26yPXv2VLoxvfzyy+2KK66w3bt3W0lJiY0YMcLatm3r/fzDDz+scMVwsrM59D8agXz77bc2ZsyYamtrt27d7L777it3fGvWrAn4H6kBAwZYly5d7Omnn7ZLLrnELrroIjv77LNt7dq19tVXX1lGRoZdeeWVAb/T6bS59dZbLSkpyV544QX79ddfvcN//fVXe+GFF6xly5Y2fPjwgOO7+eabbeTIkXbCCSfY0qVLfT5fuXJluQfuTnamToPCl19+aXFxcTZw4ED75z//aQ0aNLABAwbYPffcYwMHDrTo6Gh7/PHHA36nk4Npp9PFrPbnhdMdsdNlbcWKFda4cWMbN26cPfzww9asWTO744477JlnnrG77rrLmjRpEnC9cboehrKtqVevns/vCRo0aOA9G2Jmtnnz5oBByuk6XL9+fb/f2hQVFVlaWppdcMEFtn79+oDzwmk7ywS7/TZzHmqczn+nB1NOt/tOQ3Qo677T7aLTPpo5C8NOx+d0OXW6XoRS1+m+xiz4aer0WMHM+TITyj/PnNQL9bimNo8XQgmnRwPCUA3bu3ev3XzzzXbSSSdZeHh4hRvT7777ztq0aWMREREWGRlpTZo0sdzcXO/njz/+uPc/+YerzgPwqnDa1kcffdTvDMShCgsLbfz48QGH9+jRwxo0aGC9evWynTt32q233ur9b8RJJ53kswM5vI9Opk1xcbFlZWVZVFSUhYWFWUxMjMXExFhYWJhFRUXZLbfc4r2s4FAZGRnWrVs37+uxxx7z+XzChAmWkZHhV8/pztRpUDA7+J/qfv36WcOGDb2XK0RGRlrXrl3tpZdeKrctTg6mnU4Xs9qfF053xKEc9K1YscLOPvtsv8tHTjzxxHJ/v+N0PQxlW9OmTRuf/1xPnz7de728mdmqVasCHoA7XYdPOeUUW7Rokd/wXbt2WVpamp1xxhkBd8JO23moYLbfZs5DTVndYOe/04Mpp9t9pyE6lHXf6XbRaR/LBBuGnY7P6XLqdL0IpW4o+xqz4Kap02MFs9CWGafHUU7qhXJcU9vHC6GE06MBYaiWvPLKKzZixIhKN5Z79uyxxYsX22uvvVbpXWQO5XRns3HjRsd3D3Ha1ur03Xff2Weffea9C08goeyIzQ5eK7ts2TJ79tln7dlnn7Vly5Z5Lwtx2uYtW7b4DXe6M3UaFA5VWlpqhYWFtnXrVp+7YVXUB6cH0xV9Z6DpcqidO3fa0qVLa3xeON0Rh7qsmZlt27bNPvzwQ1uxYoX3EqiK7Nmzx5YsWRL0euh0/b355ptt1qxZ5X4+ceJE6927d5W/r7J1+C9/+Uu5/x0tKiqyLl26BDxwq852VnX7beYs1BwqmPnv9GDK6Xa/Jtb7su8tb913ul0MZd9WJpgw7HR8TpdTp+tFKHWrY18T7D8YnHC6zDjdflfHdv9QVTmuqe3jhVDC6dHAY2ZW17f3Rs1bv369oqKi1KJFi7puyhHnSJk2mzZtUsuWLR0/b6OoqEirVq1SYWGhJCkhIUGpqalq1KhRuXUKCgo0Y8YMv2fNXHbZZRo0aJDCw8PLrbt371598MEHKi4u1tlnn+19XhHKd6QsazVpw4YNiomJUfPmzavl+3755Rdt3bpVp556asDPd+/erVWrVikjIyOo763udh7up59+0vr161VaWqrmzZurVatWNTKeQL777jv99ttvateunSIiqvdxgrW93oe6XawOr776qt5++22NHj1aJ5xwQq2Ou7zlNJT1ItR1ysm+5nA1OU1raplxuv2uie1+XRwvHMsIQzVkx44d+vTTT3XGGWfouOOO0/bt2zV79mwVFxfrqquuUvv27QPW27Nnj5599tmAD5a8+uqrFRsbW6Pt3rdvnxYtWuR9GN7ll19e7jid9tFpvddee00rV67URRddpLS0NC1btkyTJ09WaWmp/vjHP+qmm26qtukgSQ8++KCuvPJKRw9ZrO22OrFy5Ur16NFDKSkpqlevnj766CNdc801Kikp0eLFi9W+fXstXrxYDRs2rJHxB7OsSdJvv/2mefPmBQxu3bt3D2rcrVu31uLFi2vs4aJOhNK/77//Xk2aNFGDBg18hu/bt095eXk677zz/OrUxTJanfOwJtVFO2tz218XD1yui2nqdF9THYLdvsHf0bK9CJWT7TeqWd2emDo2ffTRR9a4cWPzeDzWtGlTW7lypaWkpNhJJ51kbdu2tXr16nlvu3ioL774whITE61JkybWt29fu+mmm+zGG2+0vn37WpMmTezEE0+s8inlkpISe+mll+z++++3p556ynbv3h2wXFpamvf+99u2bbPTTjvNoqKi7KSTTrKYmBhr2bJlwHvOO+2j03ozZsywiIgIS01NtUaNGtnTTz9tDRs2tCFDhtjNN99s9erVK/cSlMmTJzt6sJvHc/BZGj169LDnnnuu0tP/obbVaTsPV9V5f8455/ic1n7qqaesS5cuZnbwx9idOnUq9/rvLVu2+Fxa9d5779mf//xnO/fcc+2aa64J+CwVp8ua2cHrn5OTky0uLs6aN29uHo/H+vTpY126dLHw8HC76qqrAl5S8J///CfgKzw83EaPHu19X5mqTtPKFBYW2t13311t/du6daudddZZFhYWZuHh4TZw4EDbtWuXz/gCXe4SyvpkZvbqq6/aXXfd5Z3PS5cutV69elnPnj3tv//9b8A6TvtYXetFmZSUlAofEOq0naG0tTq3/VXpo9MHLjvtXyjTtDLlrVNO9zVmB+/q9uijj9qgQYPsoosusl69etmgQYNs1qxZ1b4vLbNlyxafdbdMSUmJvfvuu9Ve7/CywWzfaqOtoS4ze/futdmzZ9v111/vvZvdrbfeam+99VaFfQuksvUpUH+qMj2dbr/roq1lqmN5OxIRhmpAjx49bMiQIVZUVGQPPPCAtWjRwoYMGeL9fPDgwXbZZZf51evWrZv1798/4EF3cXGxXX311datW7eA43S6IT70utMbb7zROnXqZAUFBWZ28HabXbt2tRtuuKHa+ui0Xvv27e3RRx81M7Nly5ZZTEyMTZs2zfv5448/bu3btw84bZyGGo/HY48//rj17dvXIiMjLS4uzm677Tb77LPPKqzntK1O23n4vO/YsWOV5n29evXsu+++874/cOCARUZGWmFhoZmZLVmyxBITE8sdZ7AHU06XNTOzXr162c033+x9YvrEiROtV69eZmb2zTffWKtWrWzcuHF+9Twej7Vo0cJatWrl8yr7DUerVq0sJSWl0mka7IFNecp7MJ3T/g0cONDOPvts+9///me5ubnWuXNnS01NtZ9//tnMyn9eSCjrk9MgFco8dLJeOA3CTtsZSludbvud9tHpA5ed9i+UaVqZ8tYpp/sap8HU6fbN6QFxKAfSTrdvtdnWUJaZ2v7nmdPp6XT7XZ1trerxQk0EtyMJYagGNG3a1L788kszO5iWw8LC7KOPPvJ+vnr1ajvxxBP96tWrV6/C//599tlnVq9evYCfOd0QH1rv5JNPttdff93n87fffttatWpVbX0MZdocevvRyMhIn1CyYcOGch8Q6jTUHDptfvzxR7vvvvusXbt2FhYWZmeddZY9+uijPnf9CbWt1dHOYOZ9cnKyvf/++973W7duNY/HY3v37vW2s7xnfzg5mHK6rJkdvBXsof/tKi4utsjISO8D5l5++eWAdW+66Sbr1KmTd5krE8ydoYKZpp988kmFr/nz5wfcYTjtX2Jios/68/vvv1vfvn2tU6dOtmPHjnJ3UKGsT06DlNM+hrJeOAnCTtsZSludbvud9tHpA5ed9i+Uaep0nXK6r3EaTJ1u35weEIdyIO10+1abbQ1lmantf545nZ5Ot9910dZQlrejAWGoBhz6IDyzg88NOPQ/8Js2bQp4kJmYmGgvv/xyud/70ksvlfufeqcb4kPvj3/CCScEvD9+oGccOO2j03otWrSw9957z8zMfvjhB/N4PD63Bn3nnXesRYsWfvXK+ugk1JR3t5b33nvPrrvuOouNjQ34cDKnba2OdgYz72+77Tbr2LGjvfHGG7Zs2TI7//zzfXbyb775prVp08avnpmzgymny5rZwXXj0EtafvnlF/N4PN7psX79+nLrvvTSS5aUlGQPP/ywd1gwYSjY9am8J61X9GA6p/2LjY31uyRi3759dtlll9npp59un376acDxhbI+OQ1STvvodL1wGoRDWdacttXptt9pH50+cDmU/oUyTZ2sU073NaEEUyfbN6cHxKEeSDvZvtVmW0NZZuryn2fBTE+n2++6aGsoy9vRgDBUA9q1a+dzn/nXX3/d+992s4PPYgh0oDFu3Dhr3LixPfDAA7ZmzRorKCiwwsJCW7NmjT3wwAPWtGnTgNdGmznfEHs8Huvdu7ddfvnl1rRpU++lT2Xy8vIsPj6+2vrotN6wYcPspJNOsn/961/2f//3f3bddddZu3bt7I033rA333zTTjvttHIvsXIaaiq7j//OnTu9/yGvjrY6bafTeb9r1y7705/+ZBEREebxeKxr164+D+JbvHixPf/88wH77uRgyumyZmZ23XXXWUZGhq1du9bWr19v/fr18znz9M4771hSUlLAumZm33//vV1wwQV20UUXWUFBQZV2GE6mabNmzWz27Nm2cePGgK9FixYF3GE47d9pp51mCxYs8BtetkNt2bJlwPGFsj45DVJO++h0vTBzFoRDWdactjWUbb+TPjp94LLT/oUyTZ2uU073NU6DqdPtm9MD4lAOpEP5J2httTWUZaYu/nnmZHo63X7XRVtDWd6OBoShGjB+/HibN29euZ+PGTPG/vjHPwb8bNKkSd5rXMPCwrz/+WrevHmFD7xyuiEeNGiQz+vwg9/bb7/devbsWW19dFpv9+7dNmTIEOvYsaNlZWVZSUmJPfDAAxYVFWUej8e6detWbnBxGmqc3sffaVtDaafTkGF28KnVgX4QWREnB1NOlzWzg/+FLnt2S1hYmLVq1cpWr17t/fyFF16wqVOnVtjm0tJSu/feey0hIaHS51s4naY9e/a0f/7zn+V+b3kPpnPav7///e+WmZkZcFz79u2zSy+9NOAOKpT1yWmQctpHp+tFmWCDcCjLWihtdbrtd9JHs4Nnb/v37x/UA5ed9i+Uaep0nXK6r3EaTJ1u35weEIdyIO10+1abbQ1lmamLf545mZ5Ot9910dZQg9uRjjBUi8oeyLZnzx77/fffKyy7fv16W7Fiha1YscLn1H55QjnQrMju3bvtt99+C7peVfp4qGCmzeHjCXRZxqGchprqVllbnbazpuZ9ZZwcTAVSNu+rsqx98803lT6MrjIrV660KVOmeK91DsTpNH3xxRftqaeeKvd7f/75Z5s7d265nwfbv3379gV86GzZNN2/f39Qd/+qyvoUSpAyC76P1bH+BhOEnbbTrHraeui2/9AztZVx0seyelV94HKo/XMyTUNdp8pT0b4mlGB6uMq2b04PiEM5kHa6fauLth6+zFTlwbaBgtShZ4qq+59nTqdndW2/a6Ot1RHcjmQ8Z6gWRUVF6ZNPPgn62QZO6x1qz549Cg8PV0xMjOPvqEl1OW1qS121tabnvZlp27ZtKi0tVbNmzRQZGRlU/aNpHpZhffL3+++/a9++fTX2XKrqsHr1ai1fvlwDBw5U06ZN67o5NWLVqlV6//33j+k+1pYNGzb4PJQyJSUl6O+obJ3av3+/9u7dW+7DLg8cOKDvv//e73l3TutVRXnbtyOhrcFso7799lsVFxeH9PDh6thmBLu/cLodro51v7rn/dGieh9NDUlSdnZ2wOEHDhzQpEmTFBcXJ0l66KGHqqVeVfz8888aN26c5syZ4zM8Pz9fTZo08W7kn376ac2YMcP78L1bb71V/fv3D/idv/32m1atWqXjjjtOHTp08Pns999/1/PPP6+BAwdWSx9DnTZr167Vhx9+qLS0NLVr105fffWV/vOf/6i4uFgDBgzQBRdcUOd9DKWdh/rll1/0xBNPeB/2d9111ykpKanSek55PB5FRUV5x5mYmKiBAwf6jTPUeehkXoRSr2xedO3aVaecckqNz4ujaX0KJCYmRjExMdqyZUvAbY0U+rwIdb1477339O2332ratGnVPi9CbWt1TZvY2Fh99dVX+utf/1or46vp/tXVOCUpJSXFLwCVt3w7XaciIiL0ww8/aOHChUH1z2m9qijveKE221od26iyh2o73ScGs81wsr+o7uOaqq77FanueX/UqOMzU8ckj8djnTp1sm7duvm8PB6PnXXWWdatWzc7//zzq61eVZT3LIYzzzzTli1bZmYHf/Rer149Gz58uM2YMcNGjBhhDRo0sNmzZ/vV+/rrry05Odl7GjojI8O2bt3q/by8O4vUxbR54403LCoqyo477jiLiYmxN954w44//njr0aOHde/e3SIiInx+aFtXfXTazubNm3vvkrN+/XpLSEiwhIQEu/DCC61FixbWuHFjW7t2bcBp45STcYYyD53OC6f1anteHE3rU2XK29Yc6/MilLbW9rQ5WsZXV+OsSHnLd21v953WC6WPtdnWULZRTtd9p/WcTpe6OK6pTHXP+6MFYagG3HvvvZaSkuK3YFT2wzan9czMXnnllQpf//73v8t9vknZbXLPPPNMv6fHP/PMM9ahQwe/epdddpldfPHF9tNPP9m3335rl1xyiaWkpHi/q7wdTV1Mm7S0NBs7dqyZmc2bN8+aNm1qY8aM8X4+ZswYu/DCC+u8j07beeh1/P3797du3brZnj17zOzg7S8vvvhiu/LKK8sdrxNOxhnKPHQ6L5zWq+15cTStT063Ncf6vAilrbU9bY6W8dXFOJ0u37W93XdaL5Q+1mZbQ9lGOV33ndZzOl3q4rimtuf90YIwVEM+/vhjO/nkk+2vf/2r90epVVnAndar6FkMhz6T4XBxcXG2cuVKMzt4m8U1a9b4fL5u3bqAz1Q44YQT7NNPP/UZNnToUGvZsqV99913Fe7canvaNGrUyL799lszMztw4IBFRET4/Jjys88+C3j3lNruo9N2HroBD7RhLe8WsqFwOk6n89DpvHBar7bnxdG0Pjnd1rhhXtT2tuZYH19djNPp8m1Wu9t9p/VC6WNttzWUbZSTdd9pvVDmRW0f19T2vD9ahNX1ZXrHqrPOOkurVq3STz/9pM6dO+uzzz6Tx+OpsXrNmzfXwoULVVpaGvC1evXqgPV69eqlGTNmSJIyMjK0YMECn8+ff/55tW3b1q/eb7/95veDxGnTpunSSy9VRkaGvvnmm2rvo9N6hwoLC1NMTIyaNGniHdawYUPt3LnTr2xd9NFJOyV5v7u4uFjx8fE+n8XHx+unn36q8rirysk4nU4Xp/MilHlYpjbmxdG0Pjnd1rhhXjhta21Pm6NxfLU1TqfLt1T7232n9ULpY222NZTp6XSfGOq+NNjpUtvHNXU5749odZ3G3GDevHkWHx9vYWFhVb7dabD1LrnkErvzzjvL/by8ZzH88MMP1qpVKzvvvPMsOzvb6tWrZ+eee67deOONdt5551lUVJTPAxXLnHXWWfbkk08GHNewYcOsSZMmVboeuzamzemnn25vvPGG9/3ht3Zdvny5paSk+NWr7T46bafH47HTTjvNzjzzTGvQoIG9+OKLPp+/++67duKJJ1bazmBUxziDmYdO54XTerU9L46m9cnptsYN86K2tzXH+vjqYpxOl+/D1fR232k9M+d9rIu2lglmG+V03Xdarzr6F2wfnY6ztuf90YK7ydWC/v3769xzz9WqVauCuu1gMPX+9re/ac+ePeV+3rZtW7399tt+wxMTE5Wfn69Jkybptddek5np448/1pYtW3TOOefogw8+UOfOnf3qXX755Zo3b56uvfZav88eeeQRlZaWaubMmdXaR6f1brnlFh04cMD7vmPHjj6fv/HGGwHvglLbfXTaznHjxvm8r1+/vs/71157Tenp6ZW2MxjVMc5g5qHTeeG0Xm3Pi6NpfXK6rXHDvKjtbc2xPr66GKfT5ftwNb3dd1pPct7HumhrmWC2UU7Xfaf1qqN/Uu0c19T2vD9a8JwhAAAAAK7Eb4YAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuBJhCAAAAIArEYYAAAAAuNL/AyCtrplbzJK7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will extract the feature importances\n",
    "feature_importances = dec_tree.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Get the names of the sorted features\n",
    "names = [mnist_features.columns[i].replace('pixel', '') for i in indices]\n",
    "\n",
    "# Number of features to display\n",
    "n_features = 50\n",
    "\n",
    "# Truncate the sorted importances and names to the top 'n_features'\n",
    "truncated_importances = feature_importances[indices][:n_features]\n",
    "truncated_names = names[:n_features]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 5))  # Adjust the figure size to avoid overcrowding\n",
    "plt.title(\"Top 50 Features: Decision Tree\")\n",
    "plt.bar(range(n_features), truncated_importances)\n",
    "plt.xticks(range(n_features), truncated_names, rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (d) Add unit testing for the feature ranking in both methods.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The test code below is commented out so that you can hit the run all button at the top of your IDE for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unittest\n",
    "\n",
    "# # We will implement a unit test to ensure we get back the correct output\n",
    "# class TestRankAndFitMethods(unittest.TestCase):\n",
    "\n",
    "#     def test_rank_feature_fldr(self):\n",
    "#         # Test if the output is a pandas Series\n",
    "#         ranked_features_df = rank_feature_fldr(mnist_df, 'label')\n",
    "#         self.assertIsInstance(ranked_features_df, pd.Series)\n",
    "\n",
    "#         # Test if the output has the correct length\n",
    "#         n_features = mnist_df.shape[1] - 1\n",
    "#         self.assertEqual(len(ranked_features_df), n_features)\n",
    "\n",
    "#     def test_fit_dec_tree_and_cross_validate(self):\n",
    "#         # Test if the output is a DecisionTreeClassifier instance\n",
    "#         decision_tree = fit_dec_tree(mnist_features, labels)\n",
    "#         self.assertIsInstance(decision_tree, DecisionTreeClassifier)\n",
    "\n",
    "#         # Test if cross_validate returns a numpy array\n",
    "#         scores = cross_validate(decision_tree, mnist_features, labels)\n",
    "#         self.assertIsInstance(scores, np.ndarray)\n",
    "\n",
    "#         # Test if cross_validate returns the correct number of scores(k=5)\n",
    "#         self.assertEqual(len(scores), 5)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (e) Plot your decision tree graph and examine the key parameters displayed. Offer\n",
    "an analysis for how the decision tree runs its predict method along with an analysis of the\n",
    "runtime complexity.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 6097 nodes and has the following tree structure:\n",
      "\n",
      "node=0 is a split node: go to node 1 if X[:, 409] <= 0.5 else to node 1942.\n",
      "\tnode=1 is a split node: go to node 2 if X[:, 434] <= 0.5 else to node 745.\n",
      "\t\tnode=2 is a split node: go to node 3 if X[:, 455] <= 5.5 else to node 442.\n",
      "\t\t\tnode=3 is a split node: go to node 4 if X[:, 323] <= 8.0 else to node 183.\n",
      "\t\t\t\tnode=4 is a split node: go to node 5 if X[:, 489] <= 1.0 else to node 116.\n"
     ]
    }
   ],
   "source": [
    "# Lets check out the node path. \n",
    "# The code/author for this block is part of the sci-kit documentation and can be found at:\n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py\n",
    "\n",
    "n_nodes = dec_tree.tree_.node_count\n",
    "children_left = dec_tree.tree_.children_left\n",
    "children_right = dec_tree.tree_.children_right\n",
    "feature = dec_tree.tree_.feature\n",
    "threshold = dec_tree.tree_.threshold\n",
    "\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, 0)]  # start with the root node id (0) and its depth (0)\n",
    "while len(stack) > 0:\n",
    "    # `pop` ensures each node is only visited once\n",
    "    node_id, depth = stack.pop()\n",
    "    node_depth[node_id] = depth\n",
    "\n",
    "    # If the left and right child of a node is not the same we have a split\n",
    "    # node\n",
    "    is_split_node = children_left[node_id] != children_right[node_id]\n",
    "    # If a split node, append left and right children and depth to `stack`\n",
    "    # so we can loop through them\n",
    "    if is_split_node:\n",
    "        stack.append((children_left[node_id], depth + 1))\n",
    "        stack.append((children_right[node_id], depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\n",
    "    \"The binary tree structure has {n} nodes and has \"\n",
    "    \"the following tree structure:\\n\".format(n=n_nodes)\n",
    ")\n",
    "for i in range(5): # Just want to show the first 50 nodes. Make n_nodes to show all\n",
    "    if is_leaves[i]:\n",
    "        print(\n",
    "            \"{space}node={node} is a leaf node.\".format(\n",
    "                space=node_depth[i] * \"\\t\", node=i\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"{space}node={node} is a split node: \"\n",
    "            \"go to node {left} if X[:, {feature}] <= {threshold} \"\n",
    "            \"else to node {right}.\".format(\n",
    "                space=node_depth[i] * \"\\t\",\n",
    "                node=i,\n",
    "                left=children_left[i],\n",
    "                feature=feature[i],\n",
    "                threshold=threshold[i],\n",
    "                right=children_right[i],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# Lets plot and look at the gini as well as the number of samples in each node\n",
    "tree.plot_tree(dec_tree, max_depth=2, fontsize=5) # Can change the max_depth to see more of the tree\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis: <br> A decision tree is a tree which there is a predict method which starts at the root of the tree and follows the branches based on the decision rules which are effectively if-elif statements (comparisons of feature values to thresholds) until it reaches a leaf node. In the tree the class label/output associated with the reached leaf node is then returned as the prediction for the input data. The value points will move all the way to the leaf nodes and will help to classify similar values. We see that the node values are moving in increments of 0.5 to seperate the data to the left or right child nodes with the gini/entropy value decreasing by about .1 at each level.  The runtime complexity (big-O) will depend on the number of features n and the depth d. The train time complexity for the tree will be O(n*d*log(n)) and the test time complexity will simply be O(d) since we will move from root to a leaf node to find values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> (f) Compare the features importances from the decision tree classifier with the feature\n",
    "ranking using FLDR. Consider what sets the two methods apart and offer a discussion on the benefits of each approach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FLDR: <br> This method measures the ratio of the between class variance to the within class variance for each feature with a higher ratio meaning higher seperability for a feature from other features. As a result this method works well with data that is relatively normal and works as a dimensionality reduction technique to extract the top features.  <br><br>\n",
    "Decision Tree Classifier: <br>\n",
    "This classifier calculates feature importances based on how much each feature contributes to the overall performance of the classifier model. The importance of each feature is determined by the average decrease in entropy/gini that results from splits made on that feature. Features with higher importance scores contribute more to the decision making process in the model. This makes them easy to interpret and visualise (as we see above) and works well with continuous variables. This type of classifier also handles interactions between features naturally as the tree structure implicity models these interactions. We can tune the number of leafs to control the fit of the model. These trees are effectively massive if-elif ladders to classify data points.   <br><br>\n",
    "Comparison: <br>\n",
    "Compared to a decison tree the FLDR is a more statistical approach that looks at the seperation between classes instead of the tree structure like in a decision tree. FLDR is less suceptable to features with more catagories when compared to decision trees because it has a more statistical approach to the seperation of classes as well. The decision tree handles non-linear relationships, and between feature interactions, where as FLDR assumes that the classes have linear seperability and doesn't account for feature interactions.<br><br>\n",
    "Note: <br>\n",
    "Interestingly when looking at the graphs we see that the two methods can provide similar features as pixel 434 was identified by both in the top two features. So we see that there is some correlation between the models, but there is not a ton of overlap."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Machine Learning<br>\n",
    "In this problem the features generated from HW2 for the numerical data set are to be used. This is\n",
    "the starting point for this problem. A minimum of 5,000 observations need to be used in the problem.\n",
    "A data set developed with the numericalFeatureGeneratorExample.m will also be provided if needed.\n",
    "The updated data is provide as an Excel file with 42,000 observations and 60 features, 20 from each\n",
    "direction. In this assignment data processing and machine learning techniques need to be combined,\n",
    "the ”best” combination is determined by the best classification accuracy:<br>\n",
    "1. Use a minimum of one of following data preprocessing methods (If more than one\n",
    "method, the processing order is up to you. Built-ins are not allowed.):<br>\n",
    "(a) Data Normalization<br>\n",
    "(b) Outlier Removal<br>\n",
    "(c) Feature Ranking and Selection<br>\n",
    "(d) Dimensionality Reduction<br><br>\n",
    "2. Use the following Machine Learning (ML) techniques (built-ins are not allowed):<br>\n",
    "(a) Bayes Classifier (built-in not allowed)<br>\n",
    "(b) Parzen Window (Gaussian kernel)<br>\n",
    "(c) Support Vector Machine using your implementation of optimization to identify the support vectors (built-ins are not allowed)<br><br>\n",
    "3. Use 5-fold cross validation on selected process from above.<br><br>\n",
    "4. Provide an analysis of your results:<br>\n",
    "(a)  What combination from the above methods gave the best results? The ”best\n",
    "results” is considered the highest classification accuracy for the 10 digits from the 5-fold\n",
    "cross validation results.<br>\n",
    "(b) Was there any part of the combination of the techniques used computationally\n",
    "expensive and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>538.267964</td>\n",
       "      <td>-314.023125</td>\n",
       "      <td>443.809967</td>\n",
       "      <td>470.780028</td>\n",
       "      <td>176.561668</td>\n",
       "      <td>-336.130920</td>\n",
       "      <td>23.221391</td>\n",
       "      <td>-45.523748</td>\n",
       "      <td>-232.436917</td>\n",
       "      <td>...</td>\n",
       "      <td>-90.463868</td>\n",
       "      <td>107.934027</td>\n",
       "      <td>25.417533</td>\n",
       "      <td>-97.235438</td>\n",
       "      <td>-66.589588</td>\n",
       "      <td>22.468479</td>\n",
       "      <td>-111.476083</td>\n",
       "      <td>62.807185</td>\n",
       "      <td>74.771969</td>\n",
       "      <td>-7.480156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-238.730010</td>\n",
       "      <td>224.609513</td>\n",
       "      <td>-197.464121</td>\n",
       "      <td>23.557813</td>\n",
       "      <td>-219.122649</td>\n",
       "      <td>-223.695514</td>\n",
       "      <td>-172.689736</td>\n",
       "      <td>125.561839</td>\n",
       "      <td>-194.150108</td>\n",
       "      <td>...</td>\n",
       "      <td>44.464226</td>\n",
       "      <td>-7.825244</td>\n",
       "      <td>-5.260700</td>\n",
       "      <td>18.905444</td>\n",
       "      <td>9.593545</td>\n",
       "      <td>-40.015688</td>\n",
       "      <td>-96.469679</td>\n",
       "      <td>24.317962</td>\n",
       "      <td>-137.943930</td>\n",
       "      <td>94.025390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>51.647165</td>\n",
       "      <td>-27.271305</td>\n",
       "      <td>-185.258708</td>\n",
       "      <td>-50.103687</td>\n",
       "      <td>216.830344</td>\n",
       "      <td>-207.152351</td>\n",
       "      <td>60.301310</td>\n",
       "      <td>95.431402</td>\n",
       "      <td>-117.051561</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.722596</td>\n",
       "      <td>-3.039582</td>\n",
       "      <td>-26.963181</td>\n",
       "      <td>-18.563280</td>\n",
       "      <td>16.095458</td>\n",
       "      <td>-81.075489</td>\n",
       "      <td>-42.589100</td>\n",
       "      <td>52.083444</td>\n",
       "      <td>-29.080312</td>\n",
       "      <td>42.344233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>110.890600</td>\n",
       "      <td>62.854265</td>\n",
       "      <td>-97.528878</td>\n",
       "      <td>-21.291295</td>\n",
       "      <td>297.976619</td>\n",
       "      <td>-32.899732</td>\n",
       "      <td>34.647964</td>\n",
       "      <td>-85.574818</td>\n",
       "      <td>-60.120835</td>\n",
       "      <td>...</td>\n",
       "      <td>23.112458</td>\n",
       "      <td>-62.558569</td>\n",
       "      <td>-115.846264</td>\n",
       "      <td>-16.405413</td>\n",
       "      <td>-37.548852</td>\n",
       "      <td>-269.649782</td>\n",
       "      <td>-130.725868</td>\n",
       "      <td>-23.526366</td>\n",
       "      <td>-20.448849</td>\n",
       "      <td>37.234782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-300.296735</td>\n",
       "      <td>152.547221</td>\n",
       "      <td>-91.949199</td>\n",
       "      <td>90.416744</td>\n",
       "      <td>-453.385929</td>\n",
       "      <td>-89.195463</td>\n",
       "      <td>-15.051828</td>\n",
       "      <td>-38.036779</td>\n",
       "      <td>-190.363654</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.107534</td>\n",
       "      <td>30.400739</td>\n",
       "      <td>-47.075128</td>\n",
       "      <td>-13.092383</td>\n",
       "      <td>-85.371052</td>\n",
       "      <td>-141.450375</td>\n",
       "      <td>-36.200100</td>\n",
       "      <td>114.188444</td>\n",
       "      <td>-21.988893</td>\n",
       "      <td>-46.480710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>0</td>\n",
       "      <td>-345.093292</td>\n",
       "      <td>11.065646</td>\n",
       "      <td>-174.747452</td>\n",
       "      <td>326.644346</td>\n",
       "      <td>-579.877065</td>\n",
       "      <td>-56.739589</td>\n",
       "      <td>284.803065</td>\n",
       "      <td>-168.864051</td>\n",
       "      <td>-222.675505</td>\n",
       "      <td>...</td>\n",
       "      <td>253.792926</td>\n",
       "      <td>-2.276109</td>\n",
       "      <td>-76.423324</td>\n",
       "      <td>16.421357</td>\n",
       "      <td>-47.923449</td>\n",
       "      <td>-164.135483</td>\n",
       "      <td>-41.600881</td>\n",
       "      <td>58.991670</td>\n",
       "      <td>-82.371978</td>\n",
       "      <td>-254.698814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>1</td>\n",
       "      <td>98.457753</td>\n",
       "      <td>-141.147284</td>\n",
       "      <td>-62.981609</td>\n",
       "      <td>14.906726</td>\n",
       "      <td>237.202744</td>\n",
       "      <td>-249.597505</td>\n",
       "      <td>194.935575</td>\n",
       "      <td>50.475505</td>\n",
       "      <td>-103.460750</td>\n",
       "      <td>...</td>\n",
       "      <td>6.134738</td>\n",
       "      <td>14.508979</td>\n",
       "      <td>-14.115238</td>\n",
       "      <td>-14.021636</td>\n",
       "      <td>59.891608</td>\n",
       "      <td>-30.775038</td>\n",
       "      <td>-10.648890</td>\n",
       "      <td>22.862916</td>\n",
       "      <td>-28.956328</td>\n",
       "      <td>-35.001724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>7</td>\n",
       "      <td>-277.428469</td>\n",
       "      <td>15.614291</td>\n",
       "      <td>-57.528770</td>\n",
       "      <td>-361.782458</td>\n",
       "      <td>176.541830</td>\n",
       "      <td>-602.188100</td>\n",
       "      <td>161.762741</td>\n",
       "      <td>-126.375074</td>\n",
       "      <td>171.504748</td>\n",
       "      <td>...</td>\n",
       "      <td>6.213266</td>\n",
       "      <td>-108.696850</td>\n",
       "      <td>-337.164756</td>\n",
       "      <td>-122.304109</td>\n",
       "      <td>146.981833</td>\n",
       "      <td>-73.174110</td>\n",
       "      <td>124.840057</td>\n",
       "      <td>-183.646347</td>\n",
       "      <td>-63.447742</td>\n",
       "      <td>-52.520125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>6</td>\n",
       "      <td>-264.146701</td>\n",
       "      <td>-242.806686</td>\n",
       "      <td>-715.897321</td>\n",
       "      <td>-53.385171</td>\n",
       "      <td>275.870094</td>\n",
       "      <td>21.959704</td>\n",
       "      <td>136.165956</td>\n",
       "      <td>370.284019</td>\n",
       "      <td>-103.682699</td>\n",
       "      <td>...</td>\n",
       "      <td>-191.564370</td>\n",
       "      <td>-343.088713</td>\n",
       "      <td>6.490805</td>\n",
       "      <td>70.169534</td>\n",
       "      <td>31.228294</td>\n",
       "      <td>32.430615</td>\n",
       "      <td>-93.090557</td>\n",
       "      <td>-34.462157</td>\n",
       "      <td>-97.924401</td>\n",
       "      <td>-33.647373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>9</td>\n",
       "      <td>154.825322</td>\n",
       "      <td>-173.127245</td>\n",
       "      <td>-246.767154</td>\n",
       "      <td>375.123163</td>\n",
       "      <td>99.948838</td>\n",
       "      <td>-167.474902</td>\n",
       "      <td>304.429111</td>\n",
       "      <td>150.721373</td>\n",
       "      <td>75.111694</td>\n",
       "      <td>...</td>\n",
       "      <td>262.355344</td>\n",
       "      <td>22.579378</td>\n",
       "      <td>99.904137</td>\n",
       "      <td>52.142862</td>\n",
       "      <td>-154.116176</td>\n",
       "      <td>-17.805026</td>\n",
       "      <td>53.552108</td>\n",
       "      <td>61.658479</td>\n",
       "      <td>7.398493</td>\n",
       "      <td>100.638849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1           2           3           4           5   \\\n",
       "0       1  538.267964 -314.023125  443.809967  470.780028  176.561668   \n",
       "1       0 -238.730010  224.609513 -197.464121   23.557813 -219.122649   \n",
       "2       1   51.647165  -27.271305 -185.258708  -50.103687  216.830344   \n",
       "3       4  110.890600   62.854265  -97.528878  -21.291295  297.976619   \n",
       "4       0 -300.296735  152.547221  -91.949199   90.416744 -453.385929   \n",
       "...    ..         ...         ...         ...         ...         ...   \n",
       "41995   0 -345.093292   11.065646 -174.747452  326.644346 -579.877065   \n",
       "41996   1   98.457753 -141.147284  -62.981609   14.906726  237.202744   \n",
       "41997   7 -277.428469   15.614291  -57.528770 -361.782458  176.541830   \n",
       "41998   6 -264.146701 -242.806686 -715.897321  -53.385171  275.870094   \n",
       "41999   9  154.825322 -173.127245 -246.767154  375.123163   99.948838   \n",
       "\n",
       "               6           7           8           9   ...          51  \\\n",
       "0     -336.130920   23.221391  -45.523748 -232.436917  ...  -90.463868   \n",
       "1     -223.695514 -172.689736  125.561839 -194.150108  ...   44.464226   \n",
       "2     -207.152351   60.301310   95.431402 -117.051561  ...  -12.722596   \n",
       "3      -32.899732   34.647964  -85.574818  -60.120835  ...   23.112458   \n",
       "4      -89.195463  -15.051828  -38.036779 -190.363654  ...  -28.107534   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "41995  -56.739589  284.803065 -168.864051 -222.675505  ...  253.792926   \n",
       "41996 -249.597505  194.935575   50.475505 -103.460750  ...    6.134738   \n",
       "41997 -602.188100  161.762741 -126.375074  171.504748  ...    6.213266   \n",
       "41998   21.959704  136.165956  370.284019 -103.682699  ... -191.564370   \n",
       "41999 -167.474902  304.429111  150.721373   75.111694  ...  262.355344   \n",
       "\n",
       "               52          53          54          55          56          57  \\\n",
       "0      107.934027   25.417533  -97.235438  -66.589588   22.468479 -111.476083   \n",
       "1       -7.825244   -5.260700   18.905444    9.593545  -40.015688  -96.469679   \n",
       "2       -3.039582  -26.963181  -18.563280   16.095458  -81.075489  -42.589100   \n",
       "3      -62.558569 -115.846264  -16.405413  -37.548852 -269.649782 -130.725868   \n",
       "4       30.400739  -47.075128  -13.092383  -85.371052 -141.450375  -36.200100   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "41995   -2.276109  -76.423324   16.421357  -47.923449 -164.135483  -41.600881   \n",
       "41996   14.508979  -14.115238  -14.021636   59.891608  -30.775038  -10.648890   \n",
       "41997 -108.696850 -337.164756 -122.304109  146.981833  -73.174110  124.840057   \n",
       "41998 -343.088713    6.490805   70.169534   31.228294   32.430615  -93.090557   \n",
       "41999   22.579378   99.904137   52.142862 -154.116176  -17.805026   53.552108   \n",
       "\n",
       "               58          59          60  \n",
       "0       62.807185   74.771969   -7.480156  \n",
       "1       24.317962 -137.943930   94.025390  \n",
       "2       52.083444  -29.080312   42.344233  \n",
       "3      -23.526366  -20.448849   37.234782  \n",
       "4      114.188444  -21.988893  -46.480710  \n",
       "...           ...         ...         ...  \n",
       "41995   58.991670  -82.371978 -254.698814  \n",
       "41996   22.862916  -28.956328  -35.001724  \n",
       "41997 -183.646347  -63.447742  -52.520125  \n",
       "41998  -34.462157  -97.924401  -33.647373  \n",
       "41999   61.658479    7.398493  100.638849  \n",
       "\n",
       "[42000 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Type the response for part 1 here ##\n",
    "# For data preprocessing we will normalize the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We will use the features generated from MATLAB feature generator\n",
    "feature_df = pd.read_csv(\"Problem2Features.csv\", header=None)\n",
    "display(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.386922</td>\n",
       "      <td>-0.799849</td>\n",
       "      <td>2.412655</td>\n",
       "      <td>1.862204</td>\n",
       "      <td>-0.095327</td>\n",
       "      <td>-0.733944</td>\n",
       "      <td>-1.028119</td>\n",
       "      <td>-0.722087</td>\n",
       "      <td>-0.666878</td>\n",
       "      <td>0.867563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.324098</td>\n",
       "      <td>0.787070</td>\n",
       "      <td>0.504785</td>\n",
       "      <td>-0.273335</td>\n",
       "      <td>-0.389655</td>\n",
       "      <td>0.508463</td>\n",
       "      <td>-0.426095</td>\n",
       "      <td>0.310356</td>\n",
       "      <td>0.637563</td>\n",
       "      <td>-0.110630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.054914</td>\n",
       "      <td>1.102631</td>\n",
       "      <td>-0.270602</td>\n",
       "      <td>-0.077722</td>\n",
       "      <td>-2.017388</td>\n",
       "      <td>-0.133239</td>\n",
       "      <td>-2.130272</td>\n",
       "      <td>0.257969</td>\n",
       "      <td>-0.409292</td>\n",
       "      <td>-2.064399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463732</td>\n",
       "      <td>0.080300</td>\n",
       "      <td>0.293978</td>\n",
       "      <td>0.573940</td>\n",
       "      <td>0.172146</td>\n",
       "      <td>0.024288</td>\n",
       "      <td>-0.308642</td>\n",
       "      <td>-0.002770</td>\n",
       "      <td>-1.236258</td>\n",
       "      <td>0.805240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.142359</td>\n",
       "      <td>0.212974</td>\n",
       "      <td>-0.219531</td>\n",
       "      <td>-0.397245</td>\n",
       "      <td>0.100281</td>\n",
       "      <td>-0.044854</td>\n",
       "      <td>-0.819515</td>\n",
       "      <td>0.085368</td>\n",
       "      <td>0.109412</td>\n",
       "      <td>0.051241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129824</td>\n",
       "      <td>0.109519</td>\n",
       "      <td>0.144848</td>\n",
       "      <td>0.300597</td>\n",
       "      <td>0.220093</td>\n",
       "      <td>-0.293874</td>\n",
       "      <td>0.113074</td>\n",
       "      <td>0.223114</td>\n",
       "      <td>-0.277275</td>\n",
       "      <td>0.338928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043822</td>\n",
       "      <td>0.531303</td>\n",
       "      <td>0.147553</td>\n",
       "      <td>-0.272265</td>\n",
       "      <td>0.494454</td>\n",
       "      <td>0.886120</td>\n",
       "      <td>-0.963836</td>\n",
       "      <td>-0.951517</td>\n",
       "      <td>0.492431</td>\n",
       "      <td>0.921698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.339061</td>\n",
       "      <td>-0.253875</td>\n",
       "      <td>-0.465917</td>\n",
       "      <td>0.316339</td>\n",
       "      <td>-0.175498</td>\n",
       "      <td>-1.755091</td>\n",
       "      <td>-0.576760</td>\n",
       "      <td>-0.392004</td>\n",
       "      <td>-0.201240</td>\n",
       "      <td>0.292826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.248397</td>\n",
       "      <td>0.848103</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.212294</td>\n",
       "      <td>-3.155336</td>\n",
       "      <td>0.585351</td>\n",
       "      <td>-1.243436</td>\n",
       "      <td>-0.679198</td>\n",
       "      <td>-0.383818</td>\n",
       "      <td>-2.097253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039993</td>\n",
       "      <td>0.313689</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>0.340508</td>\n",
       "      <td>-0.528156</td>\n",
       "      <td>-0.761705</td>\n",
       "      <td>0.163080</td>\n",
       "      <td>0.728364</td>\n",
       "      <td>-0.214807</td>\n",
       "      <td>-0.462526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41995</th>\n",
       "      <td>-1.389178</td>\n",
       "      <td>0.348382</td>\n",
       "      <td>-0.175549</td>\n",
       "      <td>1.236984</td>\n",
       "      <td>-3.769775</td>\n",
       "      <td>0.758752</td>\n",
       "      <td>0.443482</td>\n",
       "      <td>-1.428636</td>\n",
       "      <td>-0.601205</td>\n",
       "      <td>0.672841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.685980</td>\n",
       "      <td>0.114180</td>\n",
       "      <td>-0.195020</td>\n",
       "      <td>0.555818</td>\n",
       "      <td>-0.252004</td>\n",
       "      <td>-0.937486</td>\n",
       "      <td>0.120808</td>\n",
       "      <td>0.279315</td>\n",
       "      <td>-0.746723</td>\n",
       "      <td>-2.341248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41996</th>\n",
       "      <td>0.004750</td>\n",
       "      <td>-0.189242</td>\n",
       "      <td>0.292108</td>\n",
       "      <td>-0.115248</td>\n",
       "      <td>0.199241</td>\n",
       "      <td>-0.271625</td>\n",
       "      <td>-0.062093</td>\n",
       "      <td>-0.172160</td>\n",
       "      <td>0.200848</td>\n",
       "      <td>-0.252834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239930</td>\n",
       "      <td>0.216662</td>\n",
       "      <td>0.233133</td>\n",
       "      <td>0.333729</td>\n",
       "      <td>0.543062</td>\n",
       "      <td>0.095892</td>\n",
       "      <td>0.363065</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>-0.276183</td>\n",
       "      <td>-0.358953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41997</th>\n",
       "      <td>-1.176530</td>\n",
       "      <td>0.364448</td>\n",
       "      <td>0.314924</td>\n",
       "      <td>-1.749222</td>\n",
       "      <td>-0.095423</td>\n",
       "      <td>-2.155400</td>\n",
       "      <td>-0.248716</td>\n",
       "      <td>-1.185239</td>\n",
       "      <td>2.050763</td>\n",
       "      <td>-0.833924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240389</td>\n",
       "      <td>-0.535573</td>\n",
       "      <td>-1.986718</td>\n",
       "      <td>-0.456217</td>\n",
       "      <td>1.185295</td>\n",
       "      <td>-0.232648</td>\n",
       "      <td>1.423518</td>\n",
       "      <td>-1.694647</td>\n",
       "      <td>-0.580019</td>\n",
       "      <td>-0.517019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41998</th>\n",
       "      <td>-1.134790</td>\n",
       "      <td>-0.548309</td>\n",
       "      <td>-2.439861</td>\n",
       "      <td>-0.411479</td>\n",
       "      <td>0.387070</td>\n",
       "      <td>1.179216</td>\n",
       "      <td>-0.392718</td>\n",
       "      <td>1.659848</td>\n",
       "      <td>0.199355</td>\n",
       "      <td>0.856273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.914413</td>\n",
       "      <td>-1.966655</td>\n",
       "      <td>0.374729</td>\n",
       "      <td>0.947923</td>\n",
       "      <td>0.331688</td>\n",
       "      <td>0.585657</td>\n",
       "      <td>-0.282194</td>\n",
       "      <td>-0.480971</td>\n",
       "      <td>-0.883725</td>\n",
       "      <td>-0.346733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41999</th>\n",
       "      <td>0.181894</td>\n",
       "      <td>-0.302197</td>\n",
       "      <td>-0.476898</td>\n",
       "      <td>1.447271</td>\n",
       "      <td>-0.467478</td>\n",
       "      <td>0.167130</td>\n",
       "      <td>0.553894</td>\n",
       "      <td>0.402094</td>\n",
       "      <td>1.402249</td>\n",
       "      <td>-0.162122</td>\n",
       "      <td>...</td>\n",
       "      <td>1.735975</td>\n",
       "      <td>0.265936</td>\n",
       "      <td>1.016623</td>\n",
       "      <td>0.816415</td>\n",
       "      <td>-1.035106</td>\n",
       "      <td>0.196393</td>\n",
       "      <td>0.865557</td>\n",
       "      <td>0.301011</td>\n",
       "      <td>0.044068</td>\n",
       "      <td>0.864912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42000 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7   \\\n",
       "0      1.386922 -0.799849  2.412655  1.862204 -0.095327 -0.733944 -1.028119   \n",
       "1     -1.054914  1.102631 -0.270602 -0.077722 -2.017388 -0.133239 -2.130272   \n",
       "2     -0.142359  0.212974 -0.219531 -0.397245  0.100281 -0.044854 -0.819515   \n",
       "3      0.043822  0.531303  0.147553 -0.272265  0.494454  0.886120 -0.963836   \n",
       "4     -1.248397  0.848103  0.170900  0.212294 -3.155336  0.585351 -1.243436   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41995 -1.389178  0.348382 -0.175549  1.236984 -3.769775  0.758752  0.443482   \n",
       "41996  0.004750 -0.189242  0.292108 -0.115248  0.199241 -0.271625 -0.062093   \n",
       "41997 -1.176530  0.364448  0.314924 -1.749222 -0.095423 -2.155400 -0.248716   \n",
       "41998 -1.134790 -0.548309 -2.439861 -0.411479  0.387070  1.179216 -0.392718   \n",
       "41999  0.181894 -0.302197 -0.476898  1.447271 -0.467478  0.167130  0.553894   \n",
       "\n",
       "             8         9         10  ...        51        52        53  \\\n",
       "0     -0.722087 -0.666878  0.867563  ... -0.324098  0.787070  0.504785   \n",
       "1      0.257969 -0.409292 -2.064399  ...  0.463732  0.080300  0.293978   \n",
       "2      0.085368  0.109412  0.051241  ...  0.129824  0.109519  0.144848   \n",
       "3     -0.951517  0.492431  0.921698  ...  0.339061 -0.253875 -0.465917   \n",
       "4     -0.679198 -0.383818 -2.097253  ...  0.039993  0.313689  0.006648   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "41995 -1.428636 -0.601205  0.672841  ...  1.685980  0.114180 -0.195020   \n",
       "41996 -0.172160  0.200848 -0.252834  ...  0.239930  0.216662  0.233133   \n",
       "41997 -1.185239  2.050763 -0.833924  ...  0.240389 -0.535573 -1.986718   \n",
       "41998  1.659848  0.199355  0.856273  ... -0.914413 -1.966655  0.374729   \n",
       "41999  0.402094  1.402249 -0.162122  ...  1.735975  0.265936  1.016623   \n",
       "\n",
       "             54        55        56        57        58        59        60  \n",
       "0     -0.273335 -0.389655  0.508463 -0.426095  0.310356  0.637563 -0.110630  \n",
       "1      0.573940  0.172146  0.024288 -0.308642 -0.002770 -1.236258  0.805240  \n",
       "2      0.300597  0.220093 -0.293874  0.113074  0.223114 -0.277275  0.338928  \n",
       "3      0.316339 -0.175498 -1.755091 -0.576760 -0.392004 -0.201240  0.292826  \n",
       "4      0.340508 -0.528156 -0.761705  0.163080  0.728364 -0.214807 -0.462526  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "41995  0.555818 -0.252004 -0.937486  0.120808  0.279315 -0.746723 -2.341248  \n",
       "41996  0.333729  0.543062  0.095892  0.363065 -0.014607 -0.276183 -0.358953  \n",
       "41997 -0.456217  1.185295 -0.232648  1.423518 -1.694647 -0.580019 -0.517019  \n",
       "41998  0.947923  0.331688  0.585657 -0.282194 -0.480971 -0.883725 -0.346733  \n",
       "41999  0.816415 -1.035106  0.196393  0.865557  0.301011  0.044068  0.864912  \n",
       "\n",
       "[42000 rows x 60 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will normalize the data\n",
    "labels = feature_df.iloc[:, 0]\n",
    "features = feature_df.drop([0], axis=1)\n",
    "\n",
    "# Normalize the data`\n",
    "normalized_df = (features - features.mean()) / features.std()\n",
    "display(normalized_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Bayes Classifier (built-in not allowed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel function and get_kernel function will be used for all subsequent ML Algorithms for problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This function will calculate and return the prior probability of each class\n",
    "def fit_naive_bayes(train_x, train_y):\n",
    "\n",
    "    # Get the unique classes and their label\n",
    "    # Counts is the number of times each class appears in the training data\n",
    "    classes, counts = np.unique(train_y, return_counts=True)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    # Initialize matrices\n",
    "    means = np.zeros((n_classes, train_x.shape[1]))\n",
    "    variances = np.zeros((n_classes, train_x.shape[1]))\n",
    "    priors = np.zeros(n_classes)\n",
    "\n",
    "    # Compute the means, variances, and priors for each class\n",
    "    for i, c in enumerate(classes):\n",
    "        train_x_c = train_x[train_y == c] # Get the training data for class c\n",
    "        means[i, :] = np.mean(train_x_c, axis=0) # axis=0 for columns\n",
    "        variances[i, :] = np.var(train_x_c, axis=0) \n",
    "        priors[i] = counts[i] / len(train_y) # Get class prior probabilities\n",
    "\n",
    "    return classes, means, variances, priors\n",
    "\n",
    "# This function will predict the class of each sample in the test set\n",
    "def predict_naive_bayes(test_x, classes, means, variances, priors):\n",
    "    n_samples = test_x.shape[0] \n",
    "    probabilities = np.zeros((n_samples, len(classes))) \n",
    "\n",
    "    # Compute the log likelihood for each class\n",
    "    for i in range(n_samples):\n",
    "        for j in range(len(classes)):\n",
    "            likelihood = (1 / np.sqrt(2*np.pi*variances[j])) * np.exp(-((test_x[i] - means[j])**2) / (2*variances[j]))\n",
    "            probabilities[i, j] = np.log(priors[j]) + np.sum(np.log(likelihood))\n",
    "\n",
    "    return classes[np.argmax(probabilities, axis=1)] # Return the class with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will calculate and return the prior probability of each class\n",
    "def bayes_classifier(train_index, test_index):\n",
    "    train_x, test_x = normalized_df.iloc[train_index, :].to_numpy(), normalized_df.iloc[test_index].to_numpy()\n",
    "    train_y, test_y = labels.iloc[train_index].to_numpy(), labels.iloc[test_index].to_numpy()\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"Bayes Classifier\")\n",
    "    print(\"Calculating... \")\n",
    "\n",
    "    classes, means, variances, priors = fit_naive_bayes(train_x, train_y)\n",
    "    predicted_classes = predict_naive_bayes(test_x, classes, means, variances, priors)\n",
    "\n",
    "    correct = np.sum(predicted_classes == test_y.flatten())\n",
    "    total = len(test_y)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f'Classification accuracy: {accuracy}%')\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 87.12%\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "accuracy = bayes_classifier(train_index=range(5000), test_index=range(5000,7500))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Parzen Window (Gaussian kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use the kernel and get kernel functions for all of our ML Algorithms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# This will define the different kernel functions\n",
    "def kernel(x1, x2, kernel_name, kernel_arg):\n",
    "    if kernel_name == 'linear': # Linear kernel\n",
    "        k = x1.T @ x2\n",
    "    elif kernel_name == 'rbf': # Radial Basis Function kernel/ Arg gamma\n",
    "        squared_diff = np.sum((x1 - x2) ** 2)\n",
    "        k = np.exp(-kernel_arg * squared_diff) \n",
    "    elif kernel_name == 'gaussian': # Gaussian kernel/ Arg std deviation\n",
    "        squared_diff = np.sum((x1 - x2) ** 2)\n",
    "        sigma = kernel_arg \n",
    "        k = np.exp(-squared_diff / (2 * sigma ** 2))\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid kernel type '{kernel_name}'\")\n",
    "    return k\n",
    "\n",
    "# This will return the kernel matrix\n",
    "def get_kernel(x1, x2, kernel_name, arg):\n",
    "    row1 = x1.shape[0]\n",
    "    row2 = x2.shape[0] \n",
    "    k = np.zeros((row1, row2))\n",
    "    for i in range(row1):\n",
    "        for j in range(row2):\n",
    "            k[i, j] = kernel(x1[i, :], x2[j, :], kernel_name, arg)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets set up our train and classify functions for parzen window estimation\n",
    "def get_parzen_predictions(train_x, train_y, test_x, kernel_name, kernel_arg):\n",
    "    \n",
    "    # Get the classes\n",
    "    unique_classes = np.unique(train_y)\n",
    "    class_count = len(unique_classes)\n",
    "\n",
    "    # Initialize the predictions matrix and loop over the models\n",
    "    p_all = np.zeros((len(test_x), class_count))\n",
    "\n",
    "    # For each class, store the training data for that class\n",
    "    for c in unique_classes:\n",
    "\n",
    "        train_class = train_x[train_y == c, :] # Get the training data for class c\n",
    "        count_c = train_class.shape[0] # Get class count\n",
    "\n",
    "        p_c = np.zeros((len(test_x), 1))\n",
    "        k_c = get_kernel(test_x, train_class, kernel_name, kernel_arg) # Get Kernel Estimate\n",
    "        p_c = (1 / count_c) * k_c.sum(axis=1)  # Parzen window estimates\n",
    "        p_all[:, c] = p_c\n",
    "\n",
    "    return np.argmax(p_all, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our parzen window classifier\n",
    "def parzen_window_classifier(train_index, test_index, kernel_name, kernel_arg):\n",
    "    train_x, test_x = normalized_df.iloc[train_index, :].to_numpy(), normalized_df.iloc[test_index].to_numpy()\n",
    "    train_y, test_y = labels.iloc[train_index].to_numpy(), labels.iloc[test_index].to_numpy()\n",
    "    \n",
    "    # print(f\"Train X Shape: {train_x.shape}\")\n",
    "    # print(f\"Train Y Shape: {train_y.shape}\")\n",
    "    # print(f\"Test X Shape: {test_x.shape}\")\n",
    "    # print(f\"Test Y Shape: {test_y.shape}\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"Parzen Window Estimation\")\n",
    "    print(\"Calculating...\")\n",
    "    \n",
    "    predicted_classes = get_parzen_predictions(train_x, train_y, test_x, kernel_name, kernel_arg)\n",
    "\n",
    "    correct = np.sum(predicted_classes == test_y.flatten())\n",
    "    total = len(test_y)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f'Kernel:{kernel_name}\\nArg:{kernel_arg}')\n",
    "    print(f'Classification accuracy: {accuracy}%')\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 92.92%\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Usage example with a list of kernel arguments to try\n",
    "list = [1] # 1 seems to be the most accurate from testing\n",
    "for i in list:\n",
    "    accuracy = parzen_window_classifier(train_index=range(5000), test_index=range(5000, 7500), kernel_name='gaussian', kernel_arg=i) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Support Vector Machine using your implementation of optimization to identify the support vectors (built-ins are not allowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type the response for part 2c (SVM) here ##\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cvxopt\n",
    "\n",
    "# This class will train the SVM\n",
    "def train_SVM(x, y, options):\n",
    "\n",
    "    kernel_name, arg, c = options['kernel'], options['arg'], options['c']\n",
    "    mu = 1e-7 # Compensates for floating point errors\n",
    "    num_data = x.shape[0]\n",
    "    \n",
    "    K = get_kernel(x, x, kernel_name, arg)\n",
    "    \n",
    "    H = np.multiply(K, np.outer(y, y))\n",
    "    H = H + (mu * np.eye(H.shape[0]))\n",
    "        \n",
    "    # Now set up the optimization problem\n",
    "    P = np.outer(y,y) * H\n",
    "    q = -np.ones(num_data)\n",
    "    G = np.vstack([-np.eye(num_data), np.eye(num_data)])\n",
    "    h = np.hstack([np.zeros(num_data), c*np.ones(num_data)])\n",
    "    A = y.reshape(1,-1)\n",
    "    b = np.array([0.])\n",
    "    # print(f\"\\nP Shape: {P.shape}\")\n",
    "    # print(f\"q Shape: {q.shape}\")\n",
    "    # print(f\"G Shape: {G.shape}\")\n",
    "    # print(f\"h Shape: {h.shape}\")\n",
    "    # print(f\"A Shape: {A.shape}\")\n",
    "    # print(f\"b Shape: {b.shape}\")\n",
    "    \n",
    "    P = cvxopt.matrix(P, tc='d')\n",
    "    q = cvxopt.matrix(q, tc='d')\n",
    "    G = cvxopt.matrix(G, tc='d')\n",
    "    h = cvxopt.matrix(h, tc='d')\n",
    "    A = cvxopt.matrix(A, tc='d')\n",
    "    b = cvxopt.matrix(b, tc='d')\n",
    "    cvxopt.solvers.options['show_progress'] = False\n",
    "    sol = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    \n",
    "    alpha = np.array(sol['x']).flatten() # Output validated with matlab    \n",
    "    epsi = 1e-3  # This is to compensate for float point arithmatic \n",
    "    \n",
    "    # Now lets get the support and boundary vector boundaries\n",
    "    sv_inx = np.where(alpha > epsi)[0]\n",
    "    \n",
    "    boundary_inx = np.where(((c - epsi) > alpha) & (alpha > epsi))[0]\n",
    "\n",
    "    if len(boundary_inx) > 0:\n",
    "        dec_boundary = np.sum(y[boundary_inx] - \n",
    "                              (H[boundary_inx][:, sv_inx] @ \n",
    "                               (alpha[sv_inx] * y[sv_inx]))) / len(boundary_inx) \n",
    "    else:\n",
    "        dec_boundary = 0\n",
    "    \n",
    "    prediction = K @ alpha + dec_boundary \n",
    "    temp = np.ones(num_data)\n",
    "    temp[np.where(prediction < 0)] = -1 # set all values less than 0 to -1\n",
    "    err = np.sum(np.abs(temp - y)) / num_data * 100\n",
    "    \n",
    "    model = {\n",
    "        'alpha': alpha[sv_inx],\n",
    "        'dec_boundary': dec_boundary,\n",
    "        'options': options,\n",
    "        'svX': x[sv_inx, :],\n",
    "        'err': err,\n",
    "        'sv_y': y[sv_inx],\n",
    "        'sv_indx': np.where(sv_inx)[0],\n",
    "        'numberSV': len(sv_inx)\n",
    "    }\n",
    "\n",
    "    return model\n",
    "\n",
    "def classify_SVM(x, model):\n",
    "    alpha = model['alpha']\n",
    "    dec_boundary = model['dec_boundary']\n",
    "    svX = model['svX']\n",
    "    kernel_name = model['options']['kernel']\n",
    "    arg = model['options']['arg']\n",
    "\n",
    "    K = get_kernel(x, svX, kernel_name, arg)\n",
    "    prediction = np.dot(K, alpha) + dec_boundary\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement an SVM classifier from scratch. This classifier will be one vs all\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def one_vs_all_SVM(train_index, test_index, kernel_name, kernel_arg, c):\n",
    "    \n",
    "    # For this implementation we will train and test on the data, and will use five fold on next iteration\n",
    "    options = {'kernel': kernel_name, 'arg': kernel_arg, 'c': c}\n",
    "    train_x, test_x = normalized_df.iloc[train_index, :].to_numpy(), normalized_df.iloc[test_index].to_numpy()\n",
    "    train_y, test_y = labels.iloc[train_index].to_numpy(), labels.iloc[test_index].to_numpy()\n",
    "\n",
    "    # print(f\"Train X Shape: {train_x.shape}\")\n",
    "    # print(f\"Train Y Shape: {train_y.shape}\")\n",
    "    # print(f\"Test X Shape: {test_x.shape}\")\n",
    "    # print(f\"Test Y Shape: {test_y.shape}\")\n",
    "\n",
    "    # Now cycle through the ten classes\n",
    "    # We will do one vs all   \n",
    "    predictions = []\n",
    "    for i in range(10):\n",
    "        temp_train_y = train_y.copy()\n",
    "        for j in range(len(temp_train_y)):\n",
    "            if temp_train_y[j] == i:\n",
    "                temp_train_y[j] = 1\n",
    "            else:\n",
    "                temp_train_y[j] = -1\n",
    "                \n",
    "        print(\"---------------------------------------------------------------\")\n",
    "        print(f\"Training SVM for digit {i}\")\n",
    "        model = train_SVM(train_x, temp_train_y, options)\n",
    "        print(f\"Training SVM for digit {i} complete\")\n",
    "\n",
    "        preds = classify_SVM(test_x, model)\n",
    "        predictions.append(preds.flatten())\n",
    "        \n",
    "        print(f\"Number of Support Vectors: {model['numberSV']}\")\n",
    "        print(f\"Dec Boundary: {model['dec_boundary']}\")\n",
    "        #print(f\"Support Vector: {model['SvX']}\") # This will display the support vector ##############################\n",
    "        \n",
    "    predictions = np.array(predictions)\n",
    "    predicted_classes = np.argmax(predictions, axis=0)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    correct = np.sum(predicted_classes == test_y.flatten())\n",
    "    total = len(test_y)\n",
    "    accuracy = correct / total * 100\n",
    "    print(f'\\nKernel:{kernel_name}\\nArg:{kernel_arg} C:{c}')\n",
    "    print(f'Classification accuracy: {accuracy}%')\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    return accuracy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2423\n",
      "Dec Boundary: -0.6266411997936175\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1570\n",
      "Dec Boundary: -0.9052800505703134\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2506\n",
      "Dec Boundary: -0.47379602395699627\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 2346\n",
      "Dec Boundary: -0.572178804977737\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 2216\n",
      "Dec Boundary: -0.6740198657905568\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 2321\n",
      "Dec Boundary: -0.5749629654092019\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2383\n",
      "Dec Boundary: -0.6501627615718507\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 2137\n",
      "Dec Boundary: -0.7361638636225516\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2372\n",
      "Dec Boundary: -0.530344579659492\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 2066\n",
      "Dec Boundary: -0.7138912633908968\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 85.0%\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = .06\n",
    "accuracy = one_vs_all_SVM(train_index=range(5000), test_index=range(5000, 7500), kernel_name='rbf', kernel_arg=i, c=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use linear kernel if you want\n",
    "# accuracy = one_vs_all_SVM(train_index=range(2000), test_index=range(2000, 4000), kernel_name='linear', kernel_arg=None, c=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use 5-fold cross validation (from HW 3) on selected process from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 87.2%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 91.9%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2088\n",
      "Dec Boundary: -0.6337415051417185\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1369\n",
      "Dec Boundary: -0.9038196452155122\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2158\n",
      "Dec Boundary: -0.4868176080723887\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 2020\n",
      "Dec Boundary: -0.5783646935067741\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 1889\n",
      "Dec Boundary: -0.6782531563109183\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 1988\n",
      "Dec Boundary: -0.5853576245976289\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2070\n",
      "Dec Boundary: -0.6495771362572523\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 1827\n",
      "Dec Boundary: -0.7373126236716385\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2036\n",
      "Dec Boundary: -0.5386722091395176\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 1786\n",
      "Dec Boundary: -0.7170917852682794\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 83.7%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 87.3%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 91.7%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2085\n",
      "Dec Boundary: -0.6266490254392907\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1382\n",
      "Dec Boundary: -0.9037672674645181\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2139\n",
      "Dec Boundary: -0.4843181741917763\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 2020\n",
      "Dec Boundary: -0.5776629015019819\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 1891\n",
      "Dec Boundary: -0.6731993411129261\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 1989\n",
      "Dec Boundary: -0.5788808091639577\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2052\n",
      "Dec Boundary: -0.6511931941475717\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 1850\n",
      "Dec Boundary: -0.7431832732417255\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2028\n",
      "Dec Boundary: -0.55323135052623\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 1794\n",
      "Dec Boundary: -0.7090997374541451\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 85.6%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 88.6%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 91.8%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2066\n",
      "Dec Boundary: -0.632787478942577\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1376\n",
      "Dec Boundary: -0.8989974242203759\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2130\n",
      "Dec Boundary: -0.4810287049959404\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 1987\n",
      "Dec Boundary: -0.5936560323851298\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 1901\n",
      "Dec Boundary: -0.669540591530006\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 1968\n",
      "Dec Boundary: -0.5743286739874625\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2040\n",
      "Dec Boundary: -0.6530670670659627\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 1832\n",
      "Dec Boundary: -0.734592375330845\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2031\n",
      "Dec Boundary: -0.5424805347301679\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 1775\n",
      "Dec Boundary: -0.7139452715338134\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 87.1%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 87.7%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 91.5%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2088\n",
      "Dec Boundary: -0.6300576903909526\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1345\n",
      "Dec Boundary: -0.9101436329582084\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2146\n",
      "Dec Boundary: -0.4886974784165168\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 2016\n",
      "Dec Boundary: -0.5754874727055781\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 1902\n",
      "Dec Boundary: -0.671775363077177\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 1971\n",
      "Dec Boundary: -0.5833296431262868\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2045\n",
      "Dec Boundary: -0.6547714256109686\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 1823\n",
      "Dec Boundary: -0.7425776847719583\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2018\n",
      "Dec Boundary: -0.5380369949374058\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 1772\n",
      "Dec Boundary: -0.7237594556124824\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 85.6%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Bayes Classifier\n",
      "Calculating... \n",
      "Classification accuracy: 89.7%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Parzen Window Estimation\n",
      "Calculating...\n",
      "Kernel:gaussian\n",
      "Arg:1\n",
      "Classification accuracy: 92.4%\n",
      "---------------------------------------------------------------\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 0\n",
      "Training SVM for digit 0 complete\n",
      "Number of Support Vectors: 2082\n",
      "Dec Boundary: -0.6321116640487552\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 1\n",
      "Training SVM for digit 1 complete\n",
      "Number of Support Vectors: 1361\n",
      "Dec Boundary: -0.9079315334256144\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 2\n",
      "Training SVM for digit 2 complete\n",
      "Number of Support Vectors: 2152\n",
      "Dec Boundary: -0.48109315833891353\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 3\n",
      "Training SVM for digit 3 complete\n",
      "Number of Support Vectors: 2049\n",
      "Dec Boundary: -0.5763167604659846\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 4\n",
      "Training SVM for digit 4 complete\n",
      "Number of Support Vectors: 1908\n",
      "Dec Boundary: -0.6818195470750282\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 5\n",
      "Training SVM for digit 5 complete\n",
      "Number of Support Vectors: 1963\n",
      "Dec Boundary: -0.5864199583167948\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 6\n",
      "Training SVM for digit 6 complete\n",
      "Number of Support Vectors: 2063\n",
      "Dec Boundary: -0.6592584232401463\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 7\n",
      "Training SVM for digit 7 complete\n",
      "Number of Support Vectors: 1846\n",
      "Dec Boundary: -0.7307607564370309\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 8\n",
      "Training SVM for digit 8 complete\n",
      "Number of Support Vectors: 2046\n",
      "Dec Boundary: -0.5423576754234798\n",
      "---------------------------------------------------------------\n",
      "Training SVM for digit 9\n",
      "Training SVM for digit 9 complete\n",
      "Number of Support Vectors: 1800\n",
      "Dec Boundary: -0.7168398098172944\n",
      "\n",
      "Kernel:rbf\n",
      "Arg:0.06 C:10\n",
      "Classification accuracy: 84.3%\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Type the response for part 3 (5-fold cross-validation) here ##\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "index = range(5000)\n",
    "\n",
    "features = normalized_df.iloc[index,:]\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=50, shuffle=True)\n",
    "\n",
    "bayes_scores = []\n",
    "parzen_scores = []\n",
    "svm_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    \n",
    "    bayes = bayes_classifier(train_index, test_index)\n",
    "    parzen = parzen_window_classifier(train_index, test_index, kernel_name='gaussian', kernel_arg=1)\n",
    "    svm = one_vs_all_SVM(train_index, test_index, kernel_name='rbf', kernel_arg=.06, c=10)\n",
    "    \n",
    "    bayes_scores.append(bayes)\n",
    "    parzen_scores.append(parzen)\n",
    "    svm_scores.append(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five Fold Cross Validation Accuracy Scores:\n",
      "---------------------------------------------------------------\n",
      "Bayes Accuracy Scores: [87.2, 87.3, 88.6, 87.7, 89.7]\n",
      "Bayes Accuracy Mean: 88.1%\n",
      "---------------------------------------------------------------\n",
      "Parzen Accuracy Scores: [91.9, 91.7, 91.8, 91.5, 92.4]\n",
      "Parzen Accuracy Mean:  91.86%\n",
      "---------------------------------------------------------------\n",
      "SVM Accuracy Scores: [83.7, 85.6, 87.1, 85.6, 84.3]\n",
      "SVM Accuracy Mean: 85.26%\n"
     ]
    }
   ],
   "source": [
    "print(\"Five Fold Cross Validation Accuracy Scores:\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(f\"Bayes Accuracy Scores: {bayes_scores}\")\n",
    "print(f\"Bayes Accuracy Mean: {np.mean(bayes_scores)}%\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(f\"Parzen Accuracy Scores: {parzen_scores}\")\n",
    "print(f\"Parzen Accuracy Mean: {np.mean(parzen_scores): .2f}%\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(f\"SVM Accuracy Scores: {svm_scores}\")\n",
    "print(f\"SVM Accuracy Mean: {np.mean(svm_scores)}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provide an analysis of your results:\n",
    "(a) [2.5 points] What combination from the above methods gave the best results? The ”best\n",
    "results” is considered the highest classification accuracy for the 10 digits from the 5-fold\n",
    "cross validation results.<br>\n",
    "(b) [2.5 points] Was there any part of the combination of the techniques used computationally\n",
    "expensive and why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) The method that returns the best result appears to be the parzen window estimation at 91.86%. Bayes Classifier was the second most accurate with 88% overall. The SVM came in last, and with further hyperparamater tuning it is possible that the accuracy of the SVM classifier could be made higher than the bayes or parzen window. I attempted to fine tune the hyperparamaters (gamma and c) for the SVM, by looping through a list of values (took me like 8 hours of searching I shouldn't have spent so much time), and even with the best hyperparamaters that I could find it does not perform as well as the parzen window algorithm or the bayes classifier. I am sure with further searching I would be able to tune in the SVM, but generally the parzen window seems to be the best option due to ease of implementation and the speed.<br><br>\n",
    "B) The SVM was by far the most computationally expensive which lines up with what we see during the computation code and the mathematics. The calculation of the support vectors is extremely computationally expensive and results in the long delay. Not only that, but it is a binary classification algorithm so by performing one vs all we have to iterate over the ten different classes. This along with the support vector calculation massively slows down computation time. I would use a built in for future use of SVM or would write the algorithm in a faster programming language. The parzen window and bayes classifiers are both relatively efficient in comparison to the SVM. The parzen window has to compute the kernel function like the SVM which can be computationally expensive since the kernel function calculates the distance between pairs of samples. Overall the bayes is the fastest algorithm. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Game Theory (Search Algorithms)<br>\n",
    "In the tic-tac-toe code provided add the following method to allow an unbeatable AI in your game.\n",
    "Implement either MiniMax or Alpha Beta to play against and allow the play to choose the skill level.<br>\n",
    "1. Best Move (Provided) - Skill Level Easy<br>\n",
    "2. Utility Based Agent and Goal Based Agent (PA1) - Skill Level Medium<br>\n",
    "3. Skill Level Hard - Implemented the MiniMax and Alpha Beta algorithm from the\n",
    "Game Theory document for the tic-tac-toe game. You will need to alter the provided pseudo\n",
    "code to input the game board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type the response for Problem 3 here, or use the separate file in the tic-tac-toe folder ##\n",
    "# The code below should also run from the folder, but I wanted to display the code here as well\n",
    "from player import Player\n",
    "from board import Board\n",
    "\n",
    "# Represents a brute-force minimax agent\n",
    "class MinimaxPlayer(Player):\n",
    "\n",
    "    # Initialize the player\n",
    "    def __init__(self, player_number):\n",
    "        super().__init__(player_number)\n",
    "\n",
    "    # Returns the next move given the current board state\n",
    "    def get_next_move(self, board: Board) -> int:\n",
    "        _, best_move = self.get_minimax(board, True) # Get the best (max) move\n",
    "        return best_move\n",
    "\n",
    "    # Recursively determine the best move for the current player \n",
    "    # I opted to go with one function to match the unit test, but could split into max and min value functions\n",
    "    # The only modification to the test file is adding results[0] to only get the best move value\n",
    "    def get_minimax(self, state: Board, is_max: bool):\n",
    "        if state.has_win(self.mark) or state.has_win(self.opponent_mark) or state.is_full():\n",
    "            return self.get_score(state), None\n",
    "        else:\n",
    "            if is_max: # Max_Value\n",
    "                v = -10\n",
    "                best_move = None\n",
    "                for space in state.get_open_spaces():\n",
    "                    next_state = state.copy()\n",
    "                    next_state.mark_space(space, self.mark)\n",
    "                    v2, _ = self.get_minimax(next_state, False)\n",
    "                    if v2 > v:\n",
    "                        v, best_move = v2, space\n",
    "                return v, best_move\n",
    "            else: # Min_Value\n",
    "                v = 10 \n",
    "                best_move = None\n",
    "                for space in state.get_open_spaces():\n",
    "                    next_state = state.copy()\n",
    "                    next_state.mark_space(space, self.opponent_mark)\n",
    "                    v2, _ = self.get_minimax(next_state, True) \n",
    "                    if v2 < v:\n",
    "                        v, best_move = v2, space\n",
    "                return v, best_move\n",
    "\n",
    "    # Returns the score for the given board state\n",
    "    def get_score(self, state: Board):\n",
    "        if state.has_win(self.mark):\n",
    "            return 10\n",
    "        elif state.has_win(self.opponent_mark):\n",
    "            return -10\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The only modification to this test class from canvas original was adding results[0] \n",
    "import unittest\n",
    "from minimax_player import MinimaxPlayer\n",
    "from board import Board\n",
    "from parameterized import parameterized\n",
    "\n",
    "\n",
    "class MinimaxPlayerTests(unittest.TestCase):\n",
    "\n",
    "    @parameterized.expand([\n",
    "        [\"XXOOOXX-X\", 7],\n",
    "        [\"X-OOO-X-X\", 7],\n",
    "        [\"O-XX--XOO\", 4],\n",
    "        [\"OOX-X-OX-\", 3],\n",
    "        [\"X-----O--\", 1],\n",
    "        [\"---------\", 0]\n",
    "    ])\n",
    "    def test_get_next_move(self, state, expected):\n",
    "        board = Board(state)\n",
    "        player = MinimaxPlayer(1)\n",
    "        result = player.get_next_move(board)\n",
    "        self.assertEqual(expected, result)\n",
    "\n",
    "    def test_get_next_move_2(self):\n",
    "        board = Board(\"XOXOOX-X-\")\n",
    "        player = MinimaxPlayer(2)\n",
    "        result = player.get_next_move(board)\n",
    "        self.assertEqual(8, result)\n",
    "\n",
    "    # The only change to this test from original on canvas was result[0]\n",
    "    @parameterized.expand([\n",
    "        [\"XXOOOXXOX\", 0],\n",
    "        [\"XXX------\", 10],\n",
    "        [\"OOO------\", -10]\n",
    "    ])\n",
    "    def test_get_minimax_for_base_case(self, state, expected):\n",
    "        board = Board(state)\n",
    "        player = MinimaxPlayer(1)\n",
    "        result = player.get_minimax(board, True)\n",
    "        self.assertEqual(expected, result[0])\n",
    "    \n",
    "    # The only change to this test from original on canvas was result[0]\n",
    "    @parameterized.expand([\n",
    "        [\"XXOOOXX-X\", True, 10],\n",
    "        [\"X-OOOXX0X\", True, 0],\n",
    "        [\"XXOOO-X-X\", False, -10]\n",
    "    ])\n",
    "    def test_get_minimax_for_recursive_case(self, state, is_max, expected):\n",
    "        board = Board(\"XXOOO-X-X\")\n",
    "        player = MinimaxPlayer(1)\n",
    "        result = player.get_minimax(board, False)\n",
    "        self.assertEqual(-10, result[0])\n",
    "\n",
    "    @parameterized.expand([\n",
    "        [\"---------\", 0],\n",
    "        [\"XXX------\", 10],\n",
    "        [\"OOO------\", -10]\n",
    "    ])\n",
    "    def test_get_score(self, state, expected):\n",
    "        board = Board(state)\n",
    "        player = MinimaxPlayer(1)\n",
    "        result = player.get_score(board)\n",
    "        self.assertEqual(expected, result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
